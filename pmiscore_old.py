# -*- coding: utf-8 -*-
"""PMIScore

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uR_w60aBjPAp3Rc-DoxU9UY-9FmT-bos

#### 构造数据
"""

import os
import sys
import random
import numpy as np
import pandas as pd
import json
import zipfile
import time
import requests
from pathlib import Path
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm.auto import tqdm  # 引入 tqdm 用于显示进度条
import io
import contextlib

# ==========================================
# MODULE 0: 环境设置与全局配置
# ==========================================

# 1. 挂载 Google Drive (Colab 专用)
try:
    from google.colab import drive
    drive.mount('/content/drive')
    BASE_DRIVE_PATH = "/content/drive/MyDrive/PMIScore"
except ImportError:
    print("Not running in Colab, using local directory.")
    BASE_DRIVE_PATH = "./PMIScore"

# 2. 读取 Secrets (Colab 专用)
try:
    from google.colab import userdata
    OPENROUTER_API_KEY = userdata.get("OPENROUTER_API_KEY")
except (ImportError, AttributeError, KeyError) as e:
    OPENROUTER_API_KEY = None
    print(f"[Warn] 无法从 userdata 读取 OPENROUTER_API_KEY: {e}。改写功能将被跳过。")

# 3. 耗时统计工具类
class ExecutionStats:
    def __init__(self):
        self.stats = {}
        self.current_start = None

    def start(self, name):
        self.current_start = time.time()
        print(f"\n[Step Start] {name}...")
        return name

    def end(self, name):
        if self.current_start is None: return
        duration = time.time() - self.current_start
        self.stats[name] = duration
        print(f"[Step Done] {name} - Taken: {duration:.2f}s")
        self.current_start = None

    def summary(self, filename="execution_summary.txt"):
        save_path = Path(BASE_DRIVE_PATH) / filename

        with io.StringIO() as buf, contextlib.redirect_stdout(buf):
            print(f"\n{'='*20} Execution Summary {'='*20}")
            print(f"{'Task Name':<40} | {'Duration (s)':<15}")
            print("-" * 60)
            total = 0
            for k, v in self.stats.items():
                print(f"{k:<40} | {v:.2f}")
                total += v
            print("-" * 60)
            print(f"{'Total Time':<40} | {total:.2f}s")
            print("=" * 60)
            summary_output = buf.getvalue()

        # Print to console
        print(summary_output)

        # Save to file
        try:
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(summary_output)
            print(f"[Info] Execution summary saved to {save_path}")
        except Exception as e:
            print(f"[Warn] Could not save execution summary to file {save_path}: {e}")

STATS = ExecutionStats()

# 4. 全局配置类
class Config:
    # 路径配置
    BASE_DIR = Path(BASE_DRIVE_PATH)
    DATA_DIR = BASE_DIR / "datasets"
    SYNTHETIC_DIR = DATA_DIR / "synthetic"
    EMPIRICAL_DIR = DATA_DIR / "empirical"

    # --- 核心训练配置 ---
    TRAIN_NEG_RATIO = 15 # Changed from 10 to 15
    NEG_IN_DIALOGUE_PROB = 0.1
    SEED = 42

    # Synthetic 配置
    SYN_SIZE = 20
    SYN_SAMPLES = 5000
    SYN_SPLIT = (0.6, 0.2, 0.2)

    # API 配置
    API_KEY = OPENROUTER_API_KEY
    API_URL = "https://openrouter.ai/api/v1/chat/completions"
    MAX_WORKERS = 20

    # Empirical 配置
    DSTC_DOWNLOAD_DIR = "DSTC_11_Track_4"
    EMP_SAMPLES = 5000
    EMP_SPLIT = (0.6, 0.2, 0.2)

    # 长度与轮数限制
    MAX_CONTEXT_CHARS = 30000
    MAX_CONTEXT_TURNS = 10

    @staticmethod
    def init_directories():
        for p in [Config.SYNTHETIC_DIR, Config.EMPIRICAL_DIR]:
            p.mkdir(parents=True, exist_ok=True)
        print(f"[Init] Directories ready at {Config.BASE_DIR}")

    @staticmethod
    def set_seed(seed=None):
        if seed is None:
            seed = Config.SEED
        random.seed(seed)
        np.random.seed(seed)

Config.set_seed()
Config.init_directories()


# ==========================================
# MODULE 1: 数据处理 (Data Processing)
# ==========================================

class SyntheticDataProcessor:
    """处理 Synthetic 数据的生成、改写和保存"""

    def __init__(self):
        self.modes = ["diagonal", "independent", "block"]
        self.prototypes = {
            "diagonal": {
                "X": [f"I feel {mood} today." for mood in ["happy","sad","angry","nervous","relaxed","bored","upset","calm","excited","tired","anxious","joyful","disappointed","satisfied","sleepy","thrilled","shy","lonely","peaceful","confused"]],
                "Y": [f"Sounds like you are {mood}." for mood in ["happy","sad","angry","nervous","relaxed","bored","upset","calm","excited","tired","anxious","joyful","disappointed","satisfied","sleepy","thrilled","shy","lonely","peaceful","confused"]]
            },
            "independent": {
                "X": [f"Have you visited {place} recently?" for place in ["New York","London","Paris","Tokyo","Berlin","Sydney","Toronto","Chicago","Boston","Los Angeles","Madrid","Rome","Beijing","Seoul","San Francisco","Shanghai","Singapore","Dublin","Amsterdam","Dubai"]],
                "Y": [f"I think {food} tastes great." for food in ["pizza","sushi","tacos","pasta","ice cream","ramen","dumplings","burgers","curry","steak","salad","sandwiches","seafood","chocolate","pancakes","fries","barbecue","soup","bread","cheese"]]
            },
            "block": {
                "X": [f"I want to travel to {city}." for city in ["New York","London","Paris","Tokyo","Berlin"]] +
                     [f"I am learning {subject}." for subject in ["math","physics","chemistry","history","biology"]] +
                     [f"I like playing {sport}." for sport in ["basketball","soccer","tennis","badminton","table tennis"]] +
                     [f"The weather is {weather} today." for weather in ["sunny","rainy","cloudy","snowy","windy"]],
                "Y": ["That sounds like an exciting destination!", "Travel can be so enriching and fun.", "I hope you have a wonderful trip!", "Planning ahead always makes trips better.", "Exploring new places is always rewarding.", "Learning new things is always valuable.", "That's great that you're expanding your knowledge.", "Keep up the good work with your studies!", "Education opens so many doors.", "It's never too late to learn something new.", "Staying active is so important for health.", "Sports are a great way to stay fit and have fun.", "Regular exercise makes such a difference.", "It's wonderful that you enjoy being active.", "Physical activity is great for both body and mind.", "Weather can really affect our mood and plans.", "I hope the weather is nice for your activities.", "Different weather brings different opportunities.", "It's always good to be prepared for any weather.", "Each type of weather has its own charm."]
            }
        }
        # 保留4种精选风格
        self.paraphrase_styles = [
            "formal and professional",
            "casual and conversational",
            "warm and friendly",
            "polite and respectful"
        ]

    def _random_marginal(self, size, alpha=1.0):
        return np.random.dirichlet([alpha]*size)

    def _generate_joint_matrix(self, size, mode="diagonal", alpha=1.0, noise=0.2):
        px = self._random_marginal(size, alpha)
        py = self._random_marginal(size, alpha)

        if mode == "diagonal":
            base = np.diag(np.random.rand(size))
        elif mode == "block":
            base = np.zeros((size, size))
            block_size = size // 4
            for k in range(4):
                s, e = k*block_size, (k+1)*block_size
                # FIX: Correctly slice both rows and columns for a square block
                base[s:e, s:e] = np.random.rand(block_size, block_size)
        else:
            base = np.random.rand(size, size)

        mat = base
        if noise > 0:
            mat += np.random.rand(size, size) * noise * mat.mean()
        mat = mat * px[:, None] * py[None, :]
        mat /= mat.sum()

        px = mat.sum(axis=1)
        py = mat.sum(axis=0)
        return mat, px, py

    def _sample_pairs(self, joint_mat, px, py, X_list, Y_list, total_samples):
        mat = np.array(joint_mat, dtype=float)
        p = mat.flatten() / mat.sum()
        indices = np.random.choice(len(p), size=total_samples, replace=True, p=p)
        Y_len = len(Y_list)

        pairs = []
        for idx in indices:
            i, j = idx // Y_len, idx % Y_len
            pairs.append({
                "x_id": int(i),
                "y_id": int(j),
                "context": X_list[i],
                "response": Y_list[j],
                "p_xy": float(joint_mat[i, j]),
                "p_x": float(px[i]),
                "p_y": float(py[j])
            })
        return pairs

    def _paraphrase_single(self, text, style):
        if not Config.API_KEY: return text

        headers = {"Authorization": f"Bearer {Config.API_KEY}", "Content-Type": "application/json"}

        content = (
            f"Rewrite the text into style: '{style}'. Keep the original meaning unchanged."
            "Output ONLY the rewritten text."
            "Rewrite: {text}"
            "Response: "
        )

        data = {
            "model": "google/gemini-2.5-flash-lite",
            "messages": [
                {"role": "user", "content": content.format(text=text)}
            ],
            "temperature": 0.8
        }
        try:
            resp = requests.post(Config.API_URL, headers=headers, json=data, timeout=15)
            if resp.status_code == 200:
                result = resp.json()["choices"][0]["message"]["content"].strip()
                if result.startswith("Paraphrased: "): result = result[13:].strip()
                return result.strip('"').strip("' ")
        except Exception: pass
        return text

    def _paraphrase_dataset(self, data, desc_prefix=""):
        if not Config.API_KEY:
            for ex in data:
                ex["context_paraphrased"] = ex["context"]
                ex["response_paraphrased"] = ex["response"]
            return data

        print(f"[Info] Paraphrasing {len(data)} samples ({desc_prefix})...")
        context_results = [None] * len(data)
        response_results = [None] * len(data)

        # 预生成 Style，确保复现性
        ctx_styles = [random.choice(self.paraphrase_styles) for _ in range(len(data))]
        rsp_styles = [random.choice(self.paraphrase_styles) for _ in range(len(data))]

        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
            # 提交 Context 任务
            ctx_futures = {executor.submit(self._paraphrase_single, ex["context"], ctx_styles[i]): i for i, ex in enumerate(data)}
            # 提交 Response 任务
            rsp_futures = {executor.submit(self._paraphrase_single, ex["response"], rsp_styles[i]): i for i, ex in enumerate(data)}

            # 添加 tqdm 进度条
            for future in tqdm(as_completed(ctx_futures), total=len(data), desc=f"  > Contexts ({desc_prefix})"):
                idx = ctx_futures[future]
                context_results[idx] = future.result() if not future.exception() else data[idx]["context"]

            for future in tqdm(as_completed(rsp_futures), total=len(data), desc=f"  > Responses ({desc_prefix})"):
                idx = rsp_futures[future]
                response_results[idx] = future.result() if not future.exception() else data[idx]["response"]

        for i, ex in enumerate(data):
            ex["context_paraphrased"] = context_results[i]
            ex["response_paraphrased"] = response_results[i]
        return data

    def _generate_negatives(self, pos_data, joint_mat, px, py, desc_prefix=""):
        all_responses = pos_data['response'].tolist()
        all_y_ids = pos_data['y_id'].tolist()

        if 'response_paraphrased' in pos_data.columns:
            all_resp_para = pos_data['response_paraphrased'].tolist()
        else:
            all_resp_para = all_responses

        context_map = defaultdict(list)
        for idx, x_val in enumerate(pos_data['x_id']):
            context_map[int(x_val)].append(idx)

        final_data = []

        # 使用 tqdm 包装 iterrows
        iterator = tqdm(pos_data.iterrows(), total=pos_data.shape[0], desc=f"  > Neg Sampling ({desc_prefix})")

        for i, row in iterator:
            pos_item = row.to_dict()
            pos_item['is_positive'] = 1
            final_data.append(pos_item)

            current_x_id = int(row['x_id'])
            current_resp = row['response']
            current_ctx = row.get('context', '') # Use .get() with default for safety
            current_ctx_para = row.get('context_paraphrased', current_ctx)

            for _ in range(Config.TRAIN_NEG_RATIO):
                target_idx = random.randint(0, len(all_responses)-1)
                neg_y_id = int(all_y_ids[target_idx])
                neg_item = {
                    'context': current_ctx,
                    'response': all_responses[target_idx],
                    'context_paraphrased': current_ctx_para,
                    'response_paraphrased': all_resp_para[target_idx],
                    'is_positive': 0,
                    'x_id': current_x_id,
                    'y_id': neg_y_id,
                    'p_xy': float(joint_mat[current_x_id, neg_y_id]),
                    'p_x': float(px[current_x_id]),
                    'p_y': float(py[neg_y_id])
                }
                final_data.append(neg_item)

        return pd.DataFrame(final_data)

    def run(self):
        print(f"\n{'='*20} Processing Synthetic Data {'='*20}")

        mode_seeds = {
            "diagonal": 0,
            "independent": 1000,
            "block": 2000
        }

        for mode in self.modes:
            STATS.start(f"Syn - {mode} - Total")
            # 强制重置 Seed
            current_seed = Config.SEED + mode_seeds.get(mode, 0)
            Config.set_seed(current_seed)
            print(f"--- Processing {mode} (Seed: {current_seed}) ---")

            mode_dir = Config.SYNTHETIC_DIR / mode
            mode_dir.mkdir(exist_ok=True)

            joint, px, py = self._generate_joint_matrix(Config.SYN_SIZE, mode=mode)

            f_train = mode_dir / "train.csv"
            f_val = mode_dir / "val.csv"
            f_test = mode_dir / "test.csv"

            if f_train.exists() and f_val.exists() and f_test.exists():
                print(f"[Info] 正样本已存在，直接加载...")
                train = pd.read_csv(f_train)
                val = pd.read_csv(f_val)
                test = pd.read_csv(f_test)
            else:
                STATS.start(f"Syn - {mode} - Gen & Para")
                print(f"[Info] 正样本不存在，开始生成与改写...")
                pairs = self._sample_pairs(joint, px, py,
                                         self.prototypes[mode]["X"], self.prototypes[mode]["Y"],
                                         Config.SYN_SAMPLES)

                df = pd.DataFrame(pairs)
                n = len(df)
                n_train = int(Config.SYN_SPLIT[0] * n)
                n_val = int(Config.SYN_SPLIT[1] * n)

                train = df.iloc[:n_train].copy()
                val = df.iloc[n_train:n_train+n_val].copy()
                test = df.iloc[n_train+n_val:].copy()

                # 带描述的 tqdm
                train = pd.DataFrame(self._paraphrase_dataset(train.to_dict('records'), desc_prefix="train"))
                val = pd.DataFrame(self._paraphrase_dataset(val.to_dict('records'), desc_prefix="val"))
                test = pd.DataFrame(self._paraphrase_dataset(test.to_dict('records'), desc_prefix="test"))

                train.to_csv(f_train, index=False)
                val.to_csv(f_val, index=False)
                test.to_csv(f_test, index=False)
                STATS.end(f"Syn - {mode} - Gen & Para")

            f_train_neg = mode_dir / "train_with_negatives.csv"
            f_val_neg = mode_dir / "val_with_negatives.csv"
            f_test_neg = mode_dir / "test_with_negatives.csv"

            if f_train_neg.exists() and f_val_neg.exists() and f_test_neg.exists():
                print(f"[Info] 负样本文件已存在，跳过重新采样。")
            else:
                STATS.start(f"Syn - {mode} - Neg Sampling")
                print(f"[Info] 正在重新采样负样本 ({Config.TRAIN_NEG_RATIO}x)...")
                self._generate_negatives(train, joint, px, py, desc_prefix="train").to_csv(f_train_neg, index=False)
                self._generate_negatives(val, joint, px, py, desc_prefix="val").to_csv(f_val_neg, index=False)
                self._generate_negatives(test, joint, px, py, desc_prefix="test").to_csv(f_test_neg, index=False)
                STATS.end(f"Syn - {mode} - Neg Sampling")

            STATS.end(f"Syn - {mode} - Total")
            print(f"[Success] {mode} 数据集 (有序版) 已更新。")


class EmpiricalDataProcessor:
    """处理 Empirical (DSTC11) 数据"""

    def __init__(self):
        self.langs = ["en", "zh"]
        self.human_eval_files = {
            "en": ["en/fed-turn/fed-turn_eval.json"],
            "zh": ["zh/KdConv/KdConv-turn_eval.json", "zh/LCCC/LCCC-turn_eval.json"]
        }

    def _download_data(self):
        STATS.start("Emp - Download")
        zip_name = "DSTC_11_Track_4.zip"
        target_dir = Path(Config.DSTC_DOWNLOAD_DIR)

        if target_dir.exists() and any(target_dir.iterdir()):
            print(f"[Info] 目录 {target_dir} 已存在且非空，跳过下载。")
            STATS.end("Emp - Download")
            return

        if not os.path.exists(zip_name):
            print("[Info] Downloading DSTC11 Data (via wget)....")
            url = "https://huggingface.co/datasets/mario-rc/dstc11.t4/resolve/main/DSTC_11_Track_4.zip"
            # wget output is shown, so user can see progress
            cmd = f"wget -c -t 5 --show-progress -O {zip_name} {url}"
            result = os.system(cmd)
            if result != 0:
                raise RuntimeError(f"Download failed with exit code {result}")
        else:
            print(f"[Info] {zip_name} 已存在，准备解压。")

        print("[Info] Extracting...")
        try:
            with zipfile.ZipFile(zip_name, "r") as zf:
                zf.extractall(".")
            print("[Info] Extraction complete.")
            os.remove(zip_name)
            print("[Info] Zip file removed to save space.")
        except zipfile.BadZipFile:
            print("[Error] Zip file is corrupted. Removing it. Please re-run.")
            os.remove(zip_name)
            raise
        STATS.end("Emp - Download")

    def _truncate_context(self, text, max_len=30000):
        if len(text) <= max_len: return text
        truncated = text[-max_len:]
        first_newline = truncated.find('\n')
        if first_newline != -1 and first_newline < 5000:
            return truncated[first_newline+1:]
        return truncated

    def _process_dialogues(self, lang):
        target_dir = Path(Config.DSTC_DOWNLOAD_DIR) / "metadata" / "train" / lang
        if not target_dir.exists():
            print(f"[Warn] Directory not found: {target_dir}")
            return []

        files = list(target_dir.rglob("*_main.csv"))
        dialogs = defaultdict(list)

        print(f"Processing {lang} files: {len(files)}")
        # 添加 tqdm
        for f in tqdm(files, desc=f"Reading {lang} CSVs"):
            try:
                df = pd.read_csv(f)
                if not {"UID", "SEG"}.issubset(df.columns): continue
                for _, row in df.iterrows():
                    uid, seg = str(row["UID"]), str(row["SEG"]).strip()
                    if not seg or seg == "nan": continue
                    parts = uid.split("-")
                    if len(parts) >= 2: dialogs[f"{parts[0]}-{parts[1]}"].append(seg)
            except Exception: pass

        pairs = []
        for dkey, turns in dialogs.items():
            if len(turns) < 2: continue
            for i in range(1, len(turns)):
                current_turns = turns[:i][-Config.MAX_CONTEXT_TURNS:]
                ctx_str = "\n".join(current_turns).strip()
                ctx_str = self._truncate_context(ctx_str, Config.MAX_CONTEXT_CHARS)
                pairs.append({"dialog_key": dkey, "context": ctx_str, "response": turns[i].strip()})

        if len(pairs) > Config.EMP_SAMPLES:
            pairs = random.sample(pairs, Config.EMP_SAMPLES)
        return pairs

    def _add_negatives(self, pos_pairs, desc_prefix=""):
        if isinstance(pos_pairs, list):
            df = pd.DataFrame(pos_pairs)
        else:
            df = pos_pairs.copy()

        all_resps = df['response'].tolist()
        dialog_resps = defaultdict(list)
        for _, row in df.iterrows():
            dialog_resps[row['dialog_key']].append(row['response'])

        data = []
        # 添加 tqdm
        iterator = tqdm(df.iterrows(), total=df.shape[0], desc=f"  > Neg Sampling ({desc_prefix})")

        for i, row in iterator:
            base_item = row.to_dict()
            base_item['is_positive'] = 1
            data.append(base_item)

            cands = [c for c in dialog_resps[row['dialog_key']] if c != row['response']]

            for _ in range(Config.TRAIN_NEG_RATIO):
                neg_item = row.to_dict()
                neg_item['is_positive'] = 0

                if cands and random.random() < Config.NEG_IN_DIALOGUE_PROB:
                    neg_item['response'] = random.choice(cands)
                else:
                    while True:
                        cand = random.choice(all_resps)
                        if cand != row['response']:
                            neg_item['response'] = cand
                            break
                data.append(neg_item)

        return pd.DataFrame(data)

    def _strip_prefix(self, text):
        s = str(text).strip()
        if len(s) > 2 and s[1] in [":", "："]: return s[2:].strip()
        return s

    def _process_human_data(self, lang):
        human_pairs = []
        base_dev = Path(Config.DSTC_DOWNLOAD_DIR) / "metadata" / "dev"
        file_list = self.human_eval_files.get(lang, [])

        for rel_path in tqdm(file_list, desc=f"Processing Human ({lang})"):
            fpath = base_dev / rel_path
            if not fpath.exists(): continue
            try:
                with open(fpath, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    if content.startswith("["): data = json.loads(content)
                    elif content.startswith("{"): data = json.loads(content).get("data", [])
                    else: data = [json.loads(l) for l in content.split('\n') if l.strip()]

                for ex in data:
                    ctx = ex.get("context") or ex.get("history") or ex.get("contexts")
                    lines = []
                    if isinstance(ctx, list):
                        for x in ctx[-Config.MAX_CONTEXT_TURNS:]:
                            val = x.get("text", x.get("utterance", "")) if isinstance(x, dict) else str(x)
                            lines.append(self._strip_prefix(val))
                        ctx_str = "\n".join(lines).strip()
                    else:
                        lines = self._strip_prefix(str(ctx)).split('\n')[-Config.MAX_CONTEXT_TURNS:]
                        ctx_str = "\n".join(lines).strip()

                    ctx_str = self._truncate_context(ctx_str, Config.MAX_CONTEXT_CHARS)
                    rsp = ex.get("response") or ex.get("target") or ex.get("reference")
                    if isinstance(rsp, list) and rsp: rsp = rsp[0]
                    rsp_str = self._strip_prefix(str(rsp))

                    if not ctx_str or not rsp_str: continue

                    item = {"context": ctx_str, "response": rsp_str, "dialog_key": f"human|{rel_path}"}
                    ann = ex.get("annotations", {})
                    for m in ["engaging", "overall", "relevant"]:
                        scores = ann.get(m) or ann.get(m.title())
                        if scores: item[f"annot_{m}_mean"] = np.mean(scores)
                    human_pairs.append(item)
            except Exception as e: print(f"[Err] {rel_path}: {e}")
        return human_pairs

    def run(self):
        print(f"\n{'='*20} Processing Empirical Data {'='*20}")
        self._download_data()

        for lang in self.langs:
            STATS.start(f"Emp - {lang} - Total")
            lang_dir = Config.EMPIRICAL_DIR / lang
            lang_dir.mkdir(exist_ok=True)
            print(f"--- Processing {lang} ---")

            f_train = lang_dir / "train.csv"
            f_val = lang_dir / "val.csv"
            f_test = lang_dir / "test.csv"

            if f_train.exists() and f_val.exists() and f_test.exists():
                print(f"[Info] 正样本已存在，直接加载...")
                train_pos = pd.read_csv(f_train)
                val_pos = pd.read_csv(f_val)
                test_pos = pd.read_csv(f_test)
            else:
                STATS.start(f"Emp - {lang} - Parse Raw")
                print(f"[Info] 正样本不存在，开始从原始数据生成...")
                pairs = self._process_dialogues(lang)
                if pairs:
                    n_train = int(Config.EMP_SPLIT[0] * len(pairs))
                    n_val = int(Config.EMP_SPLIT[1] * len(pairs))

                    train_pos = pd.DataFrame(pairs[:n_train])
                    val_pos = pd.DataFrame(pairs[n_train:n_train+n_val])
                    test_pos = pd.DataFrame(pairs[n_train+n_val:])

                    train_pos.to_csv(f_train, index=False)
                    val_pos.to_csv(f_val, index=False)
                    test_pos.to_csv(f_test, index=False)
                    print(f"[Info] 正样本已保存。")
                else:
                    print(f"[Warn] 未找到 {lang} 的有效对话数据。")
                    train_pos, val_pos, test_pos = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
                STATS.end(f"Emp - {lang} - Parse Raw")

            f_train_neg = lang_dir / "train_with_negatives.csv"
            f_val_neg = lang_dir / "val_with_negatives.csv"
            f_test_neg = lang_dir / "test_with_negatives.csv"

            if not train_pos.empty:
                if f_train_neg.exists() and f_val_neg.exists() and f_test_neg.exists():
                    print(f"[Info] 负样本文件已存在，跳过重新采样。")
                else:
                    STATS.start(f"Emp - {lang} - Neg Sampling")
                    print(f"[Info] 正在重新采样负样本 ({Config.TRAIN_NEG_RATIO}x)...")
                    self._add_negatives(train_pos, desc_prefix="train").to_csv(f_train_neg, index=False)
                    self._add_negatives(val_pos, desc_prefix="val").to_csv(f_val_neg, index=False)
                    self._add_negatives(test_pos, desc_prefix="test").to_csv(f_test_neg, index=False)
                    STATS.end(f"Emp - {lang} - Neg Sampling")
                print(f"[Success] {lang} 训练/验证/测试集已更新。")

            human_data = self._process_human_data(lang)
            if human_data: pd.DataFrame(human_data).to_csv(lang_dir / "human.csv", index=False)
            STATS.end(f"Emp - {lang} - Total")

if __name__ == "__main__":
    # 如果 Synthetic 已经跑完，您可以注释掉下面这行，只跑 Empirical
    SyntheticDataProcessor().run()

    EmpiricalDataProcessor().run()

    # 打印最终的时间统计
    STATS.summary("execution_summary_module1.txt")
    print("\n[Module 1 Finished] Data processed.")

"""#### 处理数据"""

import os
import gc
import re
import json
import time
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm import tqdm
import torch
import io
import contextlib

# ==========================================
# CONFIGURATION
# ==========================================

# 1. 基础路径配置
try:
    from google.colab import drive
    drive.mount('/content/drive')
    BASE_DRIVE_PATH = "/content/drive/MyDrive/PMIScore"
except ImportError:
    print("Not running in Colab, using local directory.")
    BASE_DRIVE_PATH = "./PMIScore"

# 2. 耗时统计工具类 (Moved here to ensure independence)
class ExecutionStats:
    def __init__(self):
        self.stats = {}
        self.current_start = None

    def start(self, name):
        self.current_start = time.time()
        print(f"\n[Step Start] {name}...")
        return name

    def end(self, name):
        if self.current_start is None: return
        duration = time.time() - self.current_start
        self.stats[name] = duration
        print(f"[Step Done] {name} - Taken: {duration:.2f}s")
        self.current_start = None

    def summary(self, filename="execution_summary.txt"):
        save_path = Path(BASE_DRIVE_PATH) / filename

        with io.StringIO() as buf, contextlib.redirect_stdout(buf):
            print(f"\n{'='*20} Execution Summary {'='*20}")
            print(f"{'Task Name':<40} | {'Duration (s)':<15}")
            print("-" * 60)
            total = 0
            for k, v in self.stats.items():
                print(f"{k:<40} | {v:.2f}")
                total += v
            print("-" * 60)
            print(f"{'Total Time':<40} | {total:.2f}s")
            print("=" * 60)
            summary_output = buf.getvalue()

        # Print to console
        print(summary_output)

        # Save to file
        try:
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(summary_output)
            print(f"[Info] Execution summary saved to {save_path}")
        except Exception as e:
            print(f"[Warn] Could not save execution summary to file {save_path}: {e}")

STATS = ExecutionStats()

class Config:
    BASE_DIR = Path(BASE_DRIVE_PATH)
    DATA_DIR = BASE_DIR / "datasets"
    # 输出目录
    EMBEDDING_DIR = BASE_DIR / "embeddings"
    SCORE_DIR = BASE_DIR / "meep_scores"

    # 指定的模型列表
    MODELS = [
        "Qwen/Qwen3-0.6B",
        "Qwen/Qwen3-1.7B",
        "Qwen/Qwen3-4B",
        "Qwen/Qwen3-8B",
        "context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16",
        "microsoft/Phi-4-mini-instruct",
    ]

    # 数据集结构
    SYNTHETIC_CASES = ["diagonal", "independent", "block"]
    EMPIRICAL_LANGS = ["en", "zh"]

    # MEEP Prompt 模板 (用于 Direct Scoring 和 Prompted Embedding)
    MEEP_PROMPT_TEMPLATE = """Score the following response given the corresponding dialogue context on a continuous scale from 0 to 100, where a score of zero means ‘disengaging’ and a score of 100 means ‘very engaging’. Assume the response immediately follows the dialogue context. Consider that engagingness of a response is defined by the following qualities: variety of response according to the context (such as responding to ‘Hi how are you?’ with ‘I feel magnificent, because I just successfully defended my PhD! How are you?’ instead of ‘Good, how are you?’), likelihood of encouraging the other participant to respond (such as ‘I love legos! I like using them to make funny things. Do you like legos?’ instead of ‘I like legos.’), likelihood of encouraging a quality response from the other participant, interestingness, specificity, and likelihood of creating a sense of belonging for the other participant.
Dialogue context: {context}
Response: {response}
Score:"""

    PMI_PROMPT_TEMPLATE = """You are an assistant skilled at evaluating the relevance of a response to a given context.
Task: Evaluate the relevance of the following response to the context.
Context: {context}
Response: {response}
Result:"""

# 初始化根目录 (子目录会在运行时动态创建)
for p in [Config.EMBEDDING_DIR, Config.SCORE_DIR]:
    p.mkdir(parents=True, exist_ok=True)

# ==========================================
# UTILS
# ==========================================

def install_dependencies():
    """检查并安装 vLLM"""
    try:
        import vllm
    except ImportError:
        print("Installing vLLM...")
        os.system("pip install vllm")

def clear_gpu_memory():
    """清理显存，防止模型切换时 OOM"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    gc.collect()

def extract_score(text):
    """从 LLM 输出中提取 0-100 的分数"""
    matches = re.findall(r"\b\d+(?:\.\d+)?\b", text)
    if not matches:
        return None
    for m in reversed(matches):
        try:
            val = float(m)
            if 0 <= val <= 100:
                return val
        except ValueError:
            continue
    return None

# ==========================================
# PROCESSING CLASS
# ==========================================

class ModelProcessor:
    def __init__(self, model_path):
        self.model_path = model_path
        self.model_name = model_path.split("/")[-1]
        self.llm = None

    def _init_vllm(self, task="generate"):
        """初始化 vLLM"""
        if self.llm is not None:
            del self.llm
            clear_gpu_memory()

        from vllm import LLM

        stat_name = f"vLLM Init - {self.model_name} ({task})"
        STATS.start(stat_name)

        print(f"[{self.model_name}] Initializing vLLM for task: {task}...")
        try:
            if task == "embed":
                self.llm = LLM(
                    model=self.model_path,
                    trust_remote_code=True,
                    dtype="auto",
                    enforce_eager=True,
                    gpu_memory_utilization=0.95,
                    task="embed",
                    max_model_len=8192  # Explicitly set max length
                )
            else:
                self.llm = LLM(
                    model=self.model_path,
                    trust_remote_code=True,
                    dtype="auto",
                    enforce_eager=True,
                    gpu_memory_utilization=0.95,
                    max_model_len=8192  # Explicitly set max length
                )
            STATS.end(stat_name)
            return True
        except Exception as e:
            STATS.end(stat_name)
            print(f"Error initializing vLLM: {e}")
            return False

    def _get_dataset_files(self):
        """
        获取所有需要处理的文件路径列表。
        返回结构包含 dataset_type (synthetic/empirical) 和 dataset_name (diagonal/en)
        以保持输出目录结构一致。
        """
        files = []

        # 1. Synthetic (Structure: datasets/synthetic/{case_name})
        for case in Config.SYNTHETIC_CASES:
            base = Config.DATA_DIR / "synthetic" / case
            if not base.exists(): continue

            # 优先读取带负样本的文件
            for split in ["train", "val", "test"]:
                fname = f"{split}_with_negatives.csv"
                if (base / fname).exists():
                    files.append({
                        "type": "synthetic",
                        "dataset_name": case,
                        "split": split,
                        "path": base / fname
                    })
                elif (base / f"{split}.csv").exists():
                    files.append({
                        "type": "synthetic",
                        "dataset_name": case,
                        "split": split,
                        "path": base / f"{split}.csv"
                    })

        # 2. Empirical (Structure: datasets/empirical/{lang_code})
        for lang in Config.EMPIRICAL_LANGS:
            base = Config.DATA_DIR / "empirical" / lang
            if not base.exists(): continue

            # Train/Val/Test
            for split in ["train", "val", "test"]:
                fname = f"{split}_with_negatives.csv"
                if (base / fname).exists():
                    files.append({
                        "type": "empirical",
                        "dataset_name": lang, # 使用 lang (en, zh) 而不是 dstc_{lang} 以保持目录一致
                        "split": split,
                        "path": base / fname
                    })
            # Human
            if (base / "human.csv").exists():
                files.append({
                    "type": "empirical",
                    "dataset_name": lang,
                    "split": "human",
                    "path": base / "human.csv"
                })

        return files

    # -------------------------------------------------
    # PART A: Embedding Generation (3 Embeddings per sample)
    # -------------------------------------------------
    def run_embeddings(self):
        """
        生成三个 Embedding: Context, Response, Prompted
        输出目录: embeddings/{type}/{dataset_name}/{model_name}/
        """
        if not self._init_vllm(task="embed"): return

        tasks = self._get_dataset_files()

        for task in tasks:
            ds_type = task['type']
            ds_name = task['dataset_name']
            split_name = task['split']
            file_path = task['path']

            # 构建输出路径：保持与 input 相同的层级结构
            out_dir = Config.EMBEDDING_DIR / ds_type / ds_name / self.model_name
            out_dir.mkdir(parents=True, exist_ok=True)

            # 定义三个输出文件名
            out_ctx_npy = out_dir / f"{split_name}_context_embeddings.npy"
            out_rsp_npy = out_dir / f"{split_name}_response_embeddings.npy"
            out_pmt_npy = out_dir / f"{split_name}_prompt_embeddings.npy"
            out_pmt_pmi_npy = out_dir / f"{split_name}_prompt_pmi_embeddings.npy"
            out_meta = out_dir / f"{split_name}_meta.csv"

            # 检查是否全部已存在
            if out_ctx_npy.exists() and out_rsp_npy.exists() and out_pmt_npy.exists() and out_meta.exists() and out_pmt_pmi_npy.exists():
                print(f"[Skip] All embeddings exist for {ds_type}/{ds_name}/{split_name}")
                continue

            op_name = f"{self.model_name} - Embed - {ds_type}/{ds_name}/{split_name}"
            STATS.start(op_name)
            print(f"Embedding {ds_type}/{ds_name} - {split_name} ...")
            df = pd.read_csv(file_path)

            # 准备文本列表
            contexts = []
            responses = []
            prompts_meep = []
            prompts_pmi = []

            for _, row in df.iterrows():
                # 优先使用 Paraphrased 文本
                c = str(row.get('context_paraphrased', row.get('context', '')))
                r = str(row.get('response_paraphrased', row.get('response', '')))

                contexts.append(c)
                responses.append(r)
                # 生成带 Prompt 的文本
                prompts_meep.append(Config.MEEP_PROMPT_TEMPLATE.format(context=c, response=r))
                prompts_pmi.append(Config.PMI_PROMPT_TEMPLATE.format(context=c, response=r))

            try:
                # 1. Context Embeddings (Raw)
                if not out_ctx_npy.exists():
                    sub_op_name = f"{op_name} - Context"
                    STATS.start(sub_op_name)
                    print("  -> Embedding Contexts...")
                    ctx_outputs = self.llm.embed(contexts)
                    ctx_embeddings = np.array([o.outputs.embedding for o in ctx_outputs])
                    np.save(out_ctx_npy, ctx_embeddings.astype(np.float32))
                    STATS.end(sub_op_name)

                # 2. Response Embeddings (Raw)
                if not out_rsp_npy.exists():
                    sub_op_name = f"{op_name} - Response"
                    STATS.start(sub_op_name)
                    print("  -> Embedding Responses...")
                    rsp_outputs = self.llm.embed(responses)
                    rsp_embeddings = np.array([o.outputs.embedding for o in rsp_outputs])
                    np.save(out_rsp_npy, rsp_embeddings.astype(np.float32))
                    STATS.end(sub_op_name)

                # 3. Prompt Embeddings (MEEP Template)
                if not out_pmt_npy.exists():
                    sub_op_name = f"{op_name} - Prompt_MEEP"
                    STATS.start(sub_op_name)
                    print("  -> Embedding Prompts_MEEP...")
                    pmt_outputs = self.llm.embed(prompts_meep)
                    pmt_embeddings = np.array([o.outputs.embedding for o in pmt_outputs])
                    np.save(out_pmt_npy, pmt_embeddings.astype(np.float32))
                    STATS.end(sub_op_name)

                # 4. Prompt Embeddings (PMI Template)
                if not out_pmt_pmi_npy.exists():
                    sub_op_name = f"{op_name} - Prompt_PMI"
                    STATS.start(sub_op_name)
                    print("  -> Embedding Prompts_PMI...")
                    pmt_pmi_outputs = self.llm.embed(prompts_pmi)
                    pmt_pmi_embeddings = np.array([o.outputs.embedding for o in pmt_pmi_outputs])
                    np.save(out_pmt_pmi_npy, pmt_pmi_embeddings.astype(np.float32))
                    STATS.end(sub_op_name)

            except AttributeError:
                print(f"[Error] Model {self.model_name} does not support embedding generation via vLLM.")
                STATS.end(op_name)
                continue
            except Exception as e:
                print(f"[Error] Embedding generation failed: {e}")
                STATS.end(op_name)
                continue

            # 保存 Meta 信息
            cols_to_exclude = ['context', 'response', 'context_paraphrased', 'response_paraphrased', 'joint_file', 'px_file', 'py_file']
            meta_cols = [c for c in df.columns if c not in cols_to_exclude]
            df[meta_cols].to_csv(out_meta, index=False)

            print(f"Saved 3 embedding sets for {ds_name}/{split_name}")
            STATS.end(op_name)

        # 释放显存
        del self.llm
        self.llm = None
        clear_gpu_memory()

    # -------------------------------------------------
    # PART B: MEEP Direct Scoring (Generation) - Modified
    # -------------------------------------------------
    def run_meep_scoring(self):
        """
        使用 MEEP Prompt 进行打分 (N=5)
        每个 Prompt 生成 5 次结果，计算平均分和标准差。
        输出目录: meep_scores/{type}/{dataset_name}/{model_name}/
        """
        if not self._init_vllm(task="generate"): return
        from vllm import SamplingParams

        # [修改点 1] 设置 n=5 生成 5 个样本，同时提高 temperature 以获得多样性
        # 如果 temperature=0，vLLM 可能会生成 5 个完全相同的副本
        sampling_params = SamplingParams(temperature=0.7, n=5, max_tokens=16)

        tasks = self._get_dataset_files()

        for task in tasks:
            ds_type = task['type']
            ds_name = task['dataset_name']
            split_name = task['split']
            file_path = task['path']

            if split_name not in ["test", "human"]:
                continue
            # 构建输出路径
            out_dir = Config.SCORE_DIR / ds_type / ds_name / self.model_name
            out_dir.mkdir(parents=True, exist_ok=True)
            out_csv = out_dir / f"{split_name}_scores.csv"

            if out_csv.exists():
                print(f"[Skip] Scores already exist: {out_csv}")
                continue

            op_name = f"{self.model_name} - Score - {ds_type}/{ds_name}/{split_name}"
            STATS.start(op_name)

            print(f"Scoring {ds_type}/{ds_name} - {split_name} (N=5) ...")
            df = pd.read_csv(file_path)

            prompts = []
            for _, row in df.iterrows():
                ctx = str(row.get('context_paraphrased', row.get('context', '')))
                rsp = str(row.get('response_paraphrased', row.get('response', '')))
                prompts.append(Config.MEEP_PROMPT_TEMPLATE.format(context=ctx, response=rsp))

            try:
                # 批量生成，vLLM 会为每个 prompt 生成 5 个 output
                outputs = self.llm.generate(prompts, sampling_params)

                # [修改点 2] 处理 1-to-N 的输出结构
                data_rows = []

                # 遍历每一个 Prompt 的结果 (RequestOutput)
                for idx, request_output in enumerate(outputs):
                    raw_texts_list = []
                    scores_list = []

                    # 遍历该 Prompt 对应的 5 个生成结果 (CompletionOutput)
                    for completion in request_output.outputs:
                        text = completion.text
                        score = extract_score(text)

                        raw_texts_list.append(text)
                        scores_list.append(score)

                    # 计算统计数据 (过滤掉解析失败的 None)
                    valid_scores = [s for s in scores_list if s is not None]

                    if valid_scores:
                        mean_score = np.mean(valid_scores)
                        std_dev = np.std(valid_scores)
                    else:
                        mean_score = None
                        std_dev = None

                    data_rows.append({
                        'idx': idx,
                        'score_mean': mean_score,       # 主要使用指标
                        'score_std': std_dev,           # 不确定性指标
                        'scores_raw': json.dumps(scores_list), # 存为 JSON 列表字符串
                        'llm_outputs_raw': json.dumps(raw_texts_list) # 存为 JSON 列表字符串
                    })

                out_df = pd.DataFrame(data_rows)
                out_df.to_csv(out_csv, index=False)
                print(f"Saved aggregated scores (N=5) to {out_csv}")
                STATS.end(op_name)

            except Exception as e:
                print(f"[Error] Scoring failed for {ds_name}/{split_name}: {e}")
                import traceback
                traceback.print_exc()
                STATS.end(op_name)

        del self.llm
        self.llm = None
        clear_gpu_memory()

# ==========================================
# MAIN EXECUTION FLOW
# ==========================================

def main():
    install_dependencies()

    print(f"\n{'='*40}")
    print(f"Pipeline Module 2: vLLM Processing")
    print(f"Output Base: {Config.BASE_DIR}")
    print(f"Structure:   {{BASE}}/{{type}}/{{dataset}}/{{model}}/...")
    print(f"Models:      {len(Config.MODELS)} models queued")
    print(f"{'='*40}\n")

    for model_path in Config.MODELS:
        print(f"\n>>> Processing Model: {model_path} <<<")
        processor = ModelProcessor(model_path)

        # 1. 生成 Embeddings (3 sets: Context, Response, Prompted)
        try:
            processor.run_embeddings()
        except Exception as e:
            print(f"Error in Embedding for {model_path}: {e}")
            clear_gpu_memory()

        # 2. 生成 MEEP 分数
        try:
            processor.run_meep_scoring()
        except Exception as e:
            print(f"Error in Scoring for {model_path}: {e}")
            clear_gpu_memory()

    print("\n[Module 2 Finished] All embeddings and scores generated.")

if __name__ == "__main__":
    main()
    STATS.summary("execution_summary_module2.txt")

"""#### 训练打分头"""

import os
import json
import math
import pickle
import time
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KernelDensity
from sklearn.metrics import roc_auc_score
import io
import contextlib

# ==========================================
# CONFIGURATION
# ==========================================

try:
    from google.colab import drive
    drive.mount('/content/drive')
    BASE_DRIVE_PATH = "/content/drive/MyDrive/PMIScore"
except ImportError:
    print("Not running in Colab, using local directory.")
    BASE_DRIVE_PATH = "./PMIScore"

class Config:
    BASE_DIR = Path(BASE_DRIVE_PATH)
    EMBEDDING_DIR = BASE_DIR / "embeddings"
    RESULT_DIR = BASE_DIR / "results"

    # --- 实验采样设置 ---
    NUM_ROUNDS = 5          # 保持 T=5
    NEG_SAMPLES_USED = 3    # 每次使用 3 个负样本

    # 原始数据每组是 1 正 + 15 负，GROUP_SIZE 必须等于 16 以匹配文件结构
    # 这里的计算 1 + (5 * 3) = 16，逻辑是正确的
    GROUP_SIZE = 1 + (NUM_ROUNDS * NEG_SAMPLES_USED)

    # 训练超参数
    EPOCHS = 100
    BATCH_SIZE = 256
    LR = 1e-3

    # KDE 参数
    KDE_PCA_DIM = 128
    KDE_BANDWIDTH = 'auto'
    KDE_INFER_BATCH_SIZE = 2048

    # 神经网络配置 (Name, Loss, InputType)
    NN_CONFIGS = [
        ("PMI_Pair", "pmi_nce", "pair"),
        ("MINE_Pair", "mine", "pair"),
        ("InfoNCE_Pair", "infonce", "pair"),

        ("PMI_Prompt", "pmi_nce", "single"),
        ("MINE_Prompt", "mine", "single"),
        ("InfoNCE_Prompt", "infonce", "single"),
    ]

    # KDE 配置 (Name, Mode)
    KDE_CONFIGS = [
        ("KDE_Pair", "pair_diff"),
        ("KDE_Prompt", "single_diff"),
        # ("KDE_Hybrid_PMI", "hybrid_marginal"),
    ]

    # 模型列表
    MODELS = [
        "context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16",
        "microsoft/Phi-4-mini-instruct",
        "Qwen/Qwen3-0.6B",
        "Qwen/Qwen3-1.7B",
        "Qwen/Qwen3-4B",
        "Qwen/Qwen3-8B",
    ]

# ==========================================
# UTILS
# ==========================================

# 耗时统计工具类
class ExecutionStats:
    def __init__(self):
        self.stats = {}
        self.current_start = None

    def start(self, name):
        print(f"\n[Step Start] {name}...")
        self.stats[name] = {"start": time.time(), "duration": 0}
        return name

    def end(self, name):
        if name not in self.stats: return
        duration = time.time() - self.stats[name]["start"]
        self.stats[name]["duration"] = duration
        print(f"[Step Done] {name} - Taken: {duration:.2f}s")

    def summary(self, filename="execution_summary.txt"):
        save_path = Path(BASE_DRIVE_PATH) / filename

        with io.StringIO() as buf, contextlib.redirect_stdout(buf):
            print(f"\n{'='*20} Execution Summary {'='*20}")
            print(f"{'Task Name':<60} | {'Duration (s)':<15}")
            print("-" * 80)
            for k in sorted(self.stats.keys()):
                v = self.stats[k]["duration"]
                print(f"{k:<60} | {v:.2f}")
            print("-" * 80)
            print("=" * 80)
            summary_output = buf.getvalue()

        print(summary_output)

        try:
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(summary_output)
            print(f"[Info] Execution summary saved to {save_path}")
        except Exception as e:
            print(f"[Warn] Could not save execution summary to file {save_path}: {e}")

STATS = ExecutionStats()

def load_split_data(ds_type, ds_name, model_name, split):
    """加载指定 split 的 Embeddings 和 Meta"""
    base = Config.EMBEDDING_DIR / ds_type / ds_name / model_name
    files = {
        "ctx": base / f"{split}_context_embeddings.npy",
        "rsp": base / f"{split}_response_embeddings.npy",
        "pmt": base / f"{split}_prompt_embeddings.npy",
        "meta": base / f"{split}_meta.csv"
    }

    if not files["meta"].exists(): return None, None

    try:
        meta = pd.read_csv(files["meta"])
        data = {}
        if files["ctx"].exists(): data["ctx"] = np.load(files["ctx"])
        if files["rsp"].exists(): data["rsp"] = np.load(files["rsp"])
        if files["pmt"].exists(): data["pmt"] = np.load(files["pmt"])
        return data, meta
    except Exception as e:
        print(f"Error loading {ds_name}/{split}: {e}")
        return None, None

def sample_negatives(data_dict, meta_df, round_num):
    """
    对数据进行下采样：每组 16 个样本保留 1 正 + 3 确定性负样本 (根据 round_num)
    """
    total_samples = len(meta_df)
    if total_samples % Config.GROUP_SIZE != 0:
        total_samples = (total_samples // Config.GROUP_SIZE) * Config.GROUP_SIZE

    indices_to_keep = []

    for i in range(0, total_samples, Config.GROUP_SIZE):
        indices_to_keep.append(i) # Positive sample is always the first one in the group

        start_relative_neg_idx_in_group = (round_num - 1) * Config.NEG_SAMPLES_USED + 1
        end_relative_neg_idx_in_group = round_num * Config.NEG_SAMPLES_USED + 1

        for j in range(start_relative_neg_idx_in_group, end_relative_neg_idx_in_group):
            indices_to_keep.append(i + j)

    indices_to_keep = np.array(indices_to_keep)
    indices_to_keep.sort()

    new_data = {}
    for k, v in data_dict.items():
        if v is not None:
            new_data[k] = v[indices_to_keep]

    new_meta = meta_df.iloc[indices_to_keep].reset_index(drop=True)
    return new_data, new_meta

def get_input_tensor(data_dict, mode):
    if mode == 'pair':
        if 'ctx' not in data_dict or 'rsp' not in data_dict: return None
        return np.hstack([data_dict['ctx'], data_dict['rsp']])
    elif mode == 'single':
        return data_dict.get('pmt')
    return None

# ==========================================
# KDE ESTIMATORS
# ==========================================

class KDEPMIEstimator:
    def __init__(self, bandwidth="auto", pca_dim=128, scale=True):
        self.bandwidth = bandwidth
        self.pca_dim = pca_dim
        self.scale = scale
        self.scaler = None
        self.pca = None
        self.kde_pos = None
        self.kde_neg = None
        self._fitted = False

    @staticmethod
    def _auto_bw(n, d):
        return (n * (d + 2) / 4.0) ** (-1.0 / (d + 4)) * np.sqrt(d)

    def _prep_xy(self, X):
        if self.scale: X = self.scaler.transform(X)
        if self.pca is not None: X = self.pca.transform(X)
        return X

    def fit(self, train_X, train_y):
        pos = train_X[train_y > 0.5]
        neg = train_X[train_y < 0.5]
        if len(pos) == 0 or len(neg) == 0: return self
        X_all = np.vstack([pos, neg])

        if self.scale: self.scaler = StandardScaler().fit(X_all)

        if self.pca_dim is not None:
            k = min(self.pca_dim, X_all.shape[1], X_all.shape[0])
            if k < X_all.shape[1]:
                X_all_t = self.scaler.transform(X_all) if self.scale else X_all
                self.pca = PCA(n_components=k).fit(X_all_t)

        pos_t = self._prep_xy(pos)
        neg_t = self._prep_xy(neg)

        if self.bandwidth == "auto":
            d = pos_t.shape[1]
            bw_p = self._auto_bw(len(pos_t), d)
            bw_n = self._auto_bw(len(neg_t), d)
        else:
            bw_p = bw_n = float(self.bandwidth)

        self.kde_pos = KernelDensity(bandwidth=max(1e-3, bw_p)).fit(pos_t)
        self.kde_neg = KernelDensity(bandwidth=max(1e-3, bw_n)).fit(neg_t)
        self._fitted = True
        return self

    def score(self, X, batch_size=2048):
        if not self._fitted: return np.zeros(len(X))
        Xt = self._prep_xy(X)

        n = len(Xt)
        scores_pos = []
        scores_neg = []

        for i in tqdm(range(0, n, batch_size), desc="KDE Score", leave=False):
            batch = Xt[i : i + batch_size]
            scores_pos.append(self.kde_pos.score_samples(batch))
            scores_neg.append(self.kde_neg.score_samples(batch))

        full_pos = np.concatenate(scores_pos)
        full_neg = np.concatenate(scores_neg)

        return (full_pos - full_neg).astype(np.float32)

class SimpleDensityEstimator:
    def __init__(self, pca_dim=128):
        self.pca_dim = pca_dim
        self.scaler = StandardScaler()
        self.pca = None
        self.kde = None

    def fit(self, X):
        X_scaled = self.scaler.fit_transform(X)
        n = min(self.pca_dim, X.shape[0], X.shape[1])
        self.pca = PCA(n_components=n)
        X_pca = self.pca.fit_transform(X_scaled)
        n_s, n_f = X_pca.shape
        bw = (n_s * (n_f + 2) / 4.)**(-1. / (n_f + 4))
        self.kde = KernelDensity(bandwidth=bw).fit(X_pca)
        return self

    def score_samples(self, X, batch_size=2048):
        X_scaled = self.scaler.transform(X)
        X_pca = self.pca.transform(X_scaled)

        n = len(X_pca)
        scores = []
        for i in tqdm(range(0, n, batch_size), desc="SimpleKDE Score", leave=False):
            batch = X_pca[i : i + batch_size]
            scores.append(self.kde.score_samples(batch))

        return np.concatenate(scores)

def fit_kde_estimators(data_dict, meta_df, mode):
    y = meta_df['is_positive'].values.astype(float)
    if mode == 'pair_diff':
        X = np.hstack([data_dict['ctx'], data_dict['rsp']])
        est = KDEPMIEstimator(bandwidth=Config.KDE_BANDWIDTH, pca_dim=Config.KDE_PCA_DIM)
        est.fit(X, y)
        return est
    elif mode == 'single_diff':
        X = data_dict['pmt']
        est = KDEPMIEstimator(bandwidth=Config.KDE_BANDWIDTH, pca_dim=Config.KDE_PCA_DIM)
        est.fit(X, y)
        return est
    elif mode == 'hybrid_marginal':
        pos_mask = (y == 1)
        estimators = {}
        estimators['joint_z'] = SimpleDensityEstimator(Config.KDE_PCA_DIM).fit(data_dict['pmt'][pos_mask])
        estimators['marg_c']  = SimpleDensityEstimator(Config.KDE_PCA_DIM).fit(data_dict['ctx'][pos_mask])
        estimators['marg_r']  = SimpleDensityEstimator(Config.KDE_PCA_DIM).fit(data_dict['rsp'][pos_mask])
        return estimators
    return np.zeros(len(data_dict['ctx']))

def infer_kde(estimator, data_dict, mode):
    if mode == 'pair_diff':
        X = np.hstack([data_dict['ctx'], data_dict['rsp']])
        return estimator.score(X, batch_size=Config.KDE_INFER_BATCH_SIZE)
    elif mode == 'single_diff':
        X = data_dict['pmt']
        return estimator.score(X, batch_size=Config.KDE_INFER_BATCH_SIZE)
    elif mode == 'hybrid_marginal':
        log_z = estimator['joint_z'].score_samples(data_dict['pmt'], batch_size=Config.KDE_INFER_BATCH_SIZE)
        log_c = estimator['marg_c'].score_samples(data_dict['ctx'], batch_size=Config.KDE_INFER_BATCH_SIZE)
        log_r = estimator['marg_r'].score_samples(data_dict['rsp'], batch_size=Config.KDE_INFER_BATCH_SIZE)
        return log_z - log_c - log_r
    return np.zeros(len(data_dict['ctx']))

# ==========================================
# NEURAL NETWORKS
# ==========================================

class ScoringHead(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256), nn.PReLU(),
            nn.Linear(256, 128), nn.PReLU(),
            nn.Linear(128, 1)
        )
    def forward(self, x): return 20.0 * torch.tanh(self.net(x) / 20.0)

class MEEPTrainingDataset(torch.utils.data.Dataset):
    def __init__(self, X_all, is_positive_labels, neg_samples_per_pos, dim):
        self.pos_samples = []
        self.neg_samples = []
        self.dim = dim

        i = 0
        while i < len(is_positive_labels):
            if is_positive_labels[i] == 1:
                pos_emb = X_all[i]
                current_neg_embs = []
                for j in range(1, neg_samples_per_pos + 1):
                    if i + j < len(is_positive_labels) and is_positive_labels[i + j] == 0:
                        current_neg_embs.append(X_all[i + j])
                    else:
                        print(f"Warning: Not enough negative samples for positive at index {i}. Expected {neg_samples_per_pos}, got {len(current_neg_embs)}")
                        break
                if len(current_neg_embs) == neg_samples_per_pos:
                    self.pos_samples.append(pos_emb)
                    self.neg_samples.append(np.array(current_neg_embs))
                i += (1 + neg_samples_per_pos)
            else:
                i += 1

    def __len__(self):
        return len(self.pos_samples)

    def __getitem__(self, idx):
        return (torch.FloatTensor(self.pos_samples[idx]),
                torch.FloatTensor(self.neg_samples[idx]))

def train_nn_head(X_all_embeddings, all_meta, loss_type, name="Model", device='cuda'):
    dim = X_all_embeddings.shape[1]
    model = ScoringHead(dim).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=Config.LR * (1024.0 / dim))

    train_dataset = MEEPTrainingDataset(X_all_embeddings, all_meta['is_positive'].values, Config.NEG_SAMPLES_USED, dim)
    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)

    model.train()
    pbar = tqdm(range(Config.EPOCHS), desc=f"Train {name}", leave=False, unit="ep")

    for _ in pbar:
        epoch_loss, steps = 0, 0

        for xp_batch, xn_batch in train_loader:
            xp_batch, xn_batch = xp_batch.to(device), xn_batch.to(device)
            B, K = xp_batch.shape[0], xn_batch.shape[1]

            sp = model(xp_batch)

            if loss_type == 'infonce':
                sn = model(xn_batch.view(-1, dim)).view(B, K)
                logits = torch.cat([sp, sn], dim=1)
                loss = F.cross_entropy(logits, torch.zeros(B, dtype=torch.long, device=device))
            else:
                xn_flat = xn_batch.view(-1, dim)
                sn = model(xn_flat)

                if loss_type == 'pmi_nce':
                    loss = -(sp.mean() - torch.exp(sn).mean())
                elif loss_type == 'mine':
                    loss = -(sp.mean() - (torch.logsumexp(sn, dim=0) - math.log(sn.size(0))))
                else:
                    raise ValueError(f"Unknown loss: {loss_type}")

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
            steps += 1

        if steps: pbar.set_postfix({'loss': f"{epoch_loss/steps:.4f}"})

    return model

def infer_nn(model, X, device='cuda'):
    model.eval()
    loader = DataLoader(TensorDataset(torch.FloatTensor(X)), batch_size=Config.BATCH_SIZE*100, shuffle=False)
    scores = []

    with torch.no_grad():
        for (b,) in tqdm(loader, desc="NN Infer", leave=False):
            scores.append(model(b.to(device)).cpu().numpy())
    return np.concatenate(scores).flatten()

# ==========================================
# MAIN
# ==========================================

def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    STATS.start("Module 3 Total")

    if not Config.EMBEDDING_DIR.exists():
        STATS.end("Module 3 Total")
        return

    ds_types = [d.name for d in Config.EMBEDDING_DIR.iterdir() if d.is_dir()]

    for ds_type in ds_types:
        ds_names = [d.name for d in (Config.EMBEDDING_DIR / ds_type).iterdir() if d.is_dir()]
        for ds_name in ds_names:
            for model_full_name in Config.MODELS:
                model_name_for_path = Path(model_full_name).name

                emb_path = None
                candidates = [
                    Config.EMBEDDING_DIR / ds_type / ds_name / model_full_name,
                    Config.EMBEDDING_DIR / ds_type / ds_name / model_name_for_path
                ]
                for p in candidates:
                    if p.exists(): emb_path = p; break
                if not emb_path: continue

                current_model_task_name = f"M3-{ds_name}-{model_name_for_path}"
                STATS.start(current_model_task_name)

                print(f"\n{'='*60}")
                print(f"Processing: {ds_type}/{ds_name} | {model_name_for_path}")
                print(f"{'='*60}")

                out_dir = Config.RESULT_DIR / ds_type / ds_name / model_name_for_path
                out_dir.mkdir(parents=True, exist_ok=True)

                tr_data_full, tr_meta_full = load_split_data(ds_type, ds_name, model_name_for_path, "train")
                val_data, val_meta = load_split_data(ds_type, ds_name, model_name_for_path, "val")
                if tr_data_full is None:
                    STATS.end(current_model_task_name)
                    continue

                # =========================================
                # LOOP OVER T=5 ROUNDS
                # =========================================
                for t in range(1, Config.NUM_ROUNDS + 1):
                    print(f"\n>>> Round {t}/{Config.NUM_ROUNDS} <<<")

                    # A. 下采样训练数据
                    tr_data_sampled, tr_meta_sampled = sample_negatives(tr_data_full, tr_meta_full, t)

                    # B. Train NNs
                    trained_nns = {}
                    nn_pbar = tqdm(Config.NN_CONFIGS, desc="Training NNs", leave=True)
                    for name, loss, in_mode in nn_pbar:
                        timer_name = f"R1-Train-{ds_name}-{model_name_for_path}-{name}"
                        if t == 1: STATS.start(timer_name)

                        model_save_path = out_dir / f"round_{t}_{name}_model.pt"

                        X_tr = get_input_tensor(tr_data_sampled, in_mode)
                        if X_tr is None:
                            if t == 1: STATS.end(timer_name)
                            continue

                        dim = X_tr.shape[1]
                        model = ScoringHead(dim).to(device)

                        if model_save_path.exists():
                            model.load_state_dict(torch.load(model_save_path, map_location=device))
                        else:
                            model = train_nn_head(X_tr, tr_meta_sampled, loss, name=name, device=device)
                            torch.save(model.state_dict(), model_save_path)

                        trained_nns[name] = (model, in_mode)

                        if t == 1: STATS.end(timer_name)


                    # C. Fit KDEs
                    trained_kdes = {}
                    kde_pbar = tqdm(Config.KDE_CONFIGS, desc="Fitting KDEs", leave=True)
                    for name, mode in kde_pbar:
                        timer_name = f"R1-Fit-{ds_name}-{model_name_for_path}-{name}"
                        if t == 1: STATS.start(timer_name)

                        pkl_save_path = out_dir / f"round_{t}_{name}_model.pkl"

                        if pkl_save_path.exists():
                            try:
                                with open(pkl_save_path, "rb") as f: estimator = pickle.load(f)
                                trained_kdes[name] = (estimator, mode)
                            except Exception as e:
                                print(f"    [Warn] Could not load pre-trained KDE {name} from {pkl_save_path}: {e}")
                        else:
                            try:
                                estimator = fit_kde_estimators(tr_data_sampled, tr_meta_sampled, mode)
                                trained_kdes[name] = (estimator, mode)
                                with open(pkl_save_path, "wb") as f: pickle.dump(estimator, f)
                            except Exception as e:
                                print(f"    [Err] {name}: {e}")

                        if t == 1: STATS.end(timer_name)


                    # D. Inference
                    splits = ["test"]
                    if ds_type == "empirical": splits.append("human")

                    for split in splits:
                        te_data, te_meta = load_split_data(ds_type, ds_name, model_name_for_path, split)
                        if te_data is None: continue

                        # 【修改点】对推理数据也进行采样，只保留当前 round 对应的 3 个负样本
                        # 确保计算 AUC 时只在 1正3负 上进行，而不是 1正15负
                        te_data_sampled, te_meta_sampled = sample_negatives(te_data, te_meta, t)

                        res_df = te_meta_sampled.copy() # 使用 sampled 后的 meta
                        drop_cols = [c for c in res_df.columns if 'context' in c or 'response' in c or 'text' in c]
                        res_df.drop(columns=drop_cols, inplace=True, errors='ignore')

                        # NN Inference
                        for name, (model, in_mode) in trained_nns.items():
                            timer_name = f"R1-Infer-{ds_name}-{model_name_for_path}-{name}-{split}"
                            if t == 1: STATS.start(timer_name)

                            try:
                                X_in = get_input_tensor(te_data_sampled, in_mode) # 使用 Sampled 数据输入
                                if X_in is not None:
                                    res_df[f"score_{name}"] = infer_nn(model, X_in, device)
                            except Exception as e:
                                print(f"    [Err] NN inference for {name} on {split}: {e}")

                            if t == 1: STATS.end(timer_name)

                        # KDE Inference
                        for name, (est, mode) in trained_kdes.items():
                            timer_name = f"R1-Infer-{ds_name}-{model_name_for_path}-{name}-{split}"
                            if t == 1: STATS.start(timer_name)

                            try:
                                res_df[f"score_{name}"] = infer_kde(est, te_data_sampled, mode) # 使用 Sampled 数据输入
                            except Exception as e:
                                print(f"    [Err] KDE inference for {name} on {split}: {e}")

                            if t == 1: STATS.end(timer_name)

                        save_name = out_dir / f"round_{t}_{split}_inference_results.csv"
                        res_df.to_csv(save_name, index=False)

                        if split == "test" and 'is_positive' in res_df.columns:
                            print(f"  -> Round {t} Test AUC Summary:")
                            labels = res_df['is_positive'].values
                            score_cols = [c for c in res_df.columns if c.startswith("score_")]
                            score_cols.sort()

                            for col in score_cols:
                                try:
                                    auc = roc_auc_score(labels, res_df[col])
                                    method_name = col.replace("score_", "")
                                    print(f"     | {method_name:<16}: {auc:.4f}")
                                except Exception as e:
                                    print(f"     [Err] AUC calculation for {col}: {e}")

                STATS.end(current_model_task_name)

    print("\n[Module 3 Finished] All rounds completed.")
    STATS.end("Module 3 Total")

if __name__ == "__main__":
    main()
    STATS.summary("execution_summary_module3.txt")

"""#### 总结结果"""

import os
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.metrics import roc_auc_score, mean_squared_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from scipy.stats import spearmanr, pearsonr
import warnings
import json

# 忽略除数和log警告
warnings.filterwarnings("ignore")

# ==========================================
# CONFIGURATION
# ==========================================

try:
    from google.colab import drive
    drive.mount('/content/drive')
    BASE_DRIVE_PATH = "/content/drive/MyDrive/PMIScore"
except ImportError:
    print("Not running in Colab, using local directory.")
    BASE_DRIVE_PATH = "./PMIScore"

class Config:
    BASE_DIR = Path(BASE_DRIVE_PATH)
    EMBEDDING_DIR = BASE_DIR / "embeddings"
    RESULT_DIR = BASE_DIR / "results"       # Module 3 output
    SCORE_DIR = BASE_DIR / "meep_scores"    # Module 2 output (Direct LLM scores)
    OUTPUT_DIR = BASE_DIR / "analysis_report"

    # Synthetic 数据集名称
    SYN_DATASETS = ["diagonal", "independent", "block"]
    # Empirical 数据集名称
    EMP_DATASETS = ["en", "zh"]

    # Replication of Config from Module 3 for sampling logic
    NUM_ROUNDS = 5
    NEG_SAMPLES_USED = 3
    GROUP_SIZE = 1 + (NUM_ROUNDS * NEG_SAMPLES_USED)

    NUM_MEEP_SCORES = 5

Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ==========================================
# HELPER FUNCTIONS
# ==========================================

def get_true_pmi(df):
    """计算 Synthetic 数据的真实 PMI: log p(xy) - log p(x) - log p(y)"""
    eps = 1e-9
    p_xy = df['p_xy'].clip(lower=eps)
    p_x = df['p_x'].clip(lower=eps)
    p_y = df['p_y'].clip(lower=eps)
    return np.log(p_xy) - np.log(p_x) - np.log(p_y)

def normalize_score(scores, feature_range=(-5, 5)):
    if len(scores) == 0 or np.all(np.isnan(scores)): return scores
    non_nan_scores = scores[~np.isnan(scores)].reshape(-1, 1)
    if len(non_nan_scores) == 0: return scores
    scaler = MinMaxScaler(feature_range=feature_range)
    normalized_non_nan = scaler.fit_transform(non_nan_scores).flatten()

    result = np.full_like(scores, np.nan)
    result[~np.isnan(scores)] = normalized_non_nan
    return result

def z_normalize_scores(scores):
    """对分数进行Z-标准化 (均值为0，标准差为1)"""
    if len(scores) == 0 or np.all(np.isnan(scores)): return scores
    mean = np.nanmean(scores)
    std = np.nanstd(scores)
    if std == 0: return np.zeros_like(scores)
    return (scores - mean) / std

def load_direct_scores(ds_type, ds_name, model_name, split):
    """加载 Module 2 生成的 Direct MEEP Scores"""
    path = Config.SCORE_DIR / ds_type / ds_name / model_name / f"{split}_scores.csv"
    if path.exists():
        try:
            df = pd.read_csv(path)
            return df['scores_raw'].values
        except Exception:
            return None
    return None

def load_full_meta(ds_type, ds_name, model_name, split):
    """Loads the full meta.csv file"""
    base = Config.EMBEDDING_DIR / ds_type / ds_name / model_name
    meta_path = base / f"{split}_meta.csv"
    if meta_path.exists():
        return pd.read_csv(meta_path)
    return None

def recreate_sampled_indices(meta_df_full, round_num, group_size, neg_samples_used):
    """Recreates the indices that were kept during sampling"""
    total_samples = len(meta_df_full)
    if total_samples % group_size != 0:
        total_samples = (total_samples // group_size) * group_size

    indices_to_keep = []

    for i in range(0, total_samples, group_size):
        indices_to_keep.append(i) # Positive sample

        start_relative_neg_idx_in_group = (round_num - 1) * neg_samples_used + 1
        end_relative_neg_idx_in_group = round_num * neg_samples_used + 1

        for j in range(start_relative_neg_idx_in_group, end_relative_neg_idx_in_group):
            if (i + j) < (i + group_size):
                indices_to_keep.append(i + j)

    return np.array(indices_to_keep)

def process_synthetic(ds_name, model_name, model_dir):
    """处理 Synthetic 数据集的单个模型结果"""
    metrics_buffer = []

    # 遍历所有 Round
    result_files = list(model_dir.glob("round_*_test_inference_results.csv"))
    if not result_files:
        return []

    full_direct_scores_raw = load_direct_scores("synthetic", ds_name, model_name, "test")
    full_test_meta = load_full_meta("synthetic", ds_name, model_name, "test")

    for f_path in result_files:
        round_id_str = f_path.name.split("_")[1]
        round_id = int(round_id_str)
        try:
            df = pd.read_csv(f_path)

            # 1. 准备 Ground Truth
            true_pmi = get_true_pmi(df)
            labels = df['is_positive'].values

            # 2. 处理 Direct_MEEP: Round 1 -> Index 0, Round 2 -> Index 1 ...
            if full_direct_scores_raw is not None and full_test_meta is not None:
                indices_from_full_data = recreate_sampled_indices(
                    full_test_meta, round_id,
                    Config.GROUP_SIZE, Config.NEG_SAMPLES_USED
                )

                if len(indices_from_full_data) == len(df):
                    raw_scores_json_for_round = full_direct_scores_raw[indices_from_full_data]

                    # === 修改逻辑开始 ===
                    # 确定要取第几个分数 (使用取模防止越界，如 Round 1 -> index 0)
                    target_idx = (round_id - 1) % Config.NUM_MEEP_SCORES

                    current_meep_scores = []
                    for json_str in raw_scores_json_for_round:
                        try:
                            scores_list = json.loads(json_str)
                            # 取指定位置的分数
                            if target_idx < len(scores_list):
                                current_meep_scores.append(scores_list[target_idx])
                            else:
                                current_meep_scores.append(np.nan)
                        except (json.JSONDecodeError, TypeError):
                            current_meep_scores.append(np.nan)

                    current_meep_scores = pd.to_numeric(current_meep_scores, errors='coerce')
                    # === 修改逻辑结束 ===

                    # Calc AUC for Direct_MEEP
                    valid_mask = ~np.isnan(current_meep_scores) & ~np.isinf(current_meep_scores)
                    if np.sum(valid_mask) >= 2 and len(np.unique(labels[valid_mask])) >= 2:
                        auc = roc_auc_score(labels[valid_mask], current_meep_scores[valid_mask])
                    else: auc = np.nan

                    # Calc PMI Metrics for Direct_MEEP
                    pos_mask = valid_mask & (labels == 1)
                    if np.sum(pos_mask) >= 2:
                        p_scores = current_meep_scores[pos_mask]
                        p_pmi = true_pmi[pos_mask]
                        p_scores = z_normalize_scores(p_scores)
                        corr_spearman, _ = spearmanr(p_pmi, p_scores)
                        corr_pearson, _ = pearsonr(p_pmi, p_scores)
                        mse = mean_squared_error(p_pmi, p_scores)
                    else:
                        corr_spearman, corr_pearson, mse = np.nan, np.nan, np.nan

                    metrics_buffer.append({
                        "Dataset_Type": "Synthetic",
                        "Dataset": ds_name,
                        "Model": model_name,
                        "Method": "Direct_MEEP",
                        "Round": round_id, # 现在直接存储整数 Round ID
                        "AUC": auc,
                        "Spearman_PMI": corr_spearman,
                        "Pearson_PMI": corr_pearson,
                        "MSE_PMI": mse
                    })
                else:
                    print(f"Warning: Direct_MEEP raw scores not available for {f_path}. Skipping.")

            # 3. 计算其他打分方法的指标
            method_cols = [c for c in df.columns if c.startswith("score_")]

            for method in method_cols:
                method_name = method.replace("score_", "")
                scores = pd.to_numeric(df[method].values, errors='coerce')

                # AUC
                valid_mask = ~np.isnan(scores) & ~np.isinf(scores)
                if np.sum(valid_mask) < 2 or len(np.unique(labels[valid_mask])) < 2: continue

                v_scores = scores[valid_mask]
                v_labels = labels[valid_mask]

                try:
                    auc = roc_auc_score(v_labels, v_scores)
                except ValueError:
                    auc = np.nan

                # PMI Metrics
                pos_mask = valid_mask & (labels == 1)
                if np.sum(pos_mask) < 2:
                    corr_spearman, corr_pearson, mse = np.nan, np.nan, np.nan
                else:
                    p_scores = scores[pos_mask]
                    p_pmi = true_pmi[pos_mask]

                    if "MEEP" in method_name:
                        p_scores = z_normalize_scores(p_scores)

                    corr_spearman, _ = spearmanr(p_pmi, p_scores)
                    corr_pearson, _ = pearsonr(p_pmi, p_scores)
                    mse = mean_squared_error(p_pmi, p_scores)

                metrics_buffer.append({
                    "Dataset_Type": "Synthetic",
                    "Dataset": ds_name,
                    "Model": model_name,
                    "Method": method_name,
                    "Round": round_id,
                    "AUC": auc,
                    "Spearman_PMI": corr_spearman,
                    "Pearson_PMI": corr_pearson,
                    "MSE_PMI": mse
                })

        except Exception as e:
            print(f"[Err] Processing {f_path}: {e}")

    return metrics_buffer

def process_empirical(ds_name, model_name, model_dir):
    """处理 Empirical 数据集的单个模型结果"""
    metrics_buffer = []

    files_map = {}
    for f in model_dir.glob("round_*_inference_results.csv"):
        parts = f.name.split("_")
        r_id = int(parts[1])
        split = parts[2]
        if r_id not in files_map: files_map[r_id] = {}
        files_map[r_id][split] = f

    full_direct_scores_raw_test = load_direct_scores("empirical", ds_name, model_name, "test")
    full_direct_scores_raw_human = load_direct_scores("empirical", ds_name, model_name, "human")
    full_test_meta = load_full_meta("empirical", ds_name, model_name, "test")
    full_human_meta = load_full_meta("empirical", ds_name, model_name, "human")

    for r_id, paths in files_map.items():
        # --- Part A: Test Set (AUC) ---
        if "test" in paths:
            try:
                df = pd.read_csv(paths["test"])
                labels = df['is_positive'].values

                if full_direct_scores_raw_test is not None and full_test_meta is not None:
                    indices_from_full_data = recreate_sampled_indices(
                        full_test_meta, r_id,
                        Config.GROUP_SIZE, Config.NEG_SAMPLES_USED
                    )

                    if len(indices_from_full_data) == len(df):
                        raw_scores_json_for_round = full_direct_scores_raw_test[indices_from_full_data]

                        # === 修改逻辑开始 ===
                        target_idx = (r_id - 1) % Config.NUM_MEEP_SCORES

                        current_meep_scores = []
                        for json_str in raw_scores_json_for_round:
                            try:
                                scores_list = json.loads(json_str)
                                if target_idx < len(scores_list):
                                    current_meep_scores.append(scores_list[target_idx])
                                else:
                                    current_meep_scores.append(np.nan)
                            except (json.JSONDecodeError, TypeError):
                                current_meep_scores.append(np.nan)

                        current_meep_scores = pd.to_numeric(current_meep_scores, errors='coerce')
                        # === 修改逻辑结束 ===

                        valid_mask = ~np.isnan(current_meep_scores) & ~np.isinf(current_meep_scores)
                        if np.sum(valid_mask) >= 2 and len(np.unique(labels[valid_mask])) >= 2:
                            auc = roc_auc_score(labels[valid_mask], current_meep_scores[valid_mask])
                        else: auc = np.nan

                        metrics_buffer.append({
                            "key": (ds_name, model_name, "Direct_MEEP", r_id),
                            "AUC": auc
                        })
                    else:
                        print(f"Warning: Direct_MEEP indices mismatch for {paths['test']}.")

                # Other methods
                method_cols = [c for c in df.columns if c.startswith("score_")]
                for method in method_cols:
                    try:
                        scores = pd.to_numeric(df[method].values, errors='coerce')
                        valid_mask = ~np.isnan(scores) & ~np.isinf(scores)
                        if np.sum(valid_mask) < 2 or len(np.unique(labels[valid_mask])) < 2: continue
                        auc = roc_auc_score(labels[valid_mask], scores[valid_mask])
                    except ValueError: auc = np.nan

                    metrics_buffer.append({
                        "key": (ds_name, model_name, method.replace("score_", ""), r_id),
                        "AUC": auc
                    })
            except Exception as e: print(f"Err Empirical Test {ds_name} R{r_id}: {e}")

        # --- Part B: Human Set (Correlation) ---
        if "human" in paths:
            try:
                df = pd.read_csv(paths["human"])
                target_eng = df.get('annot_engaging_mean')
                target_rel = df.get('annot_relevant_mean')

                if full_direct_scores_raw_human is not None and full_human_meta is not None:
                    indices_from_full_data = recreate_sampled_indices(
                        full_human_meta, r_id,
                        Config.GROUP_SIZE, Config.NEG_SAMPLES_USED
                    )

                    if len(indices_from_full_data) == len(df):
                        raw_scores_json_for_round = full_direct_scores_raw_human[indices_from_full_data]

                        # === 修改逻辑开始 ===
                        target_idx = (r_id - 1) % Config.NUM_MEEP_SCORES

                        current_meep_scores = []
                        for json_str in raw_scores_json_for_round:
                            try:
                                scores_list = json.loads(json_str)
                                if target_idx < len(scores_list):
                                    current_meep_scores.append(scores_list[target_idx])
                                else:
                                    current_meep_scores.append(np.nan)
                            except (json.JSONDecodeError, TypeError):
                                current_meep_scores.append(np.nan)

                        current_meep_scores = pd.to_numeric(current_meep_scores, errors='coerce')
                        # === 修改逻辑结束 ===

                        res = {}
                        if target_eng is not None:
                            mask = ~np.isnan(current_meep_scores) & ~np.isinf(current_meep_scores) & ~np.isnan(target_eng)
                            res['Spearman_Engaging'] = spearmanr(target_eng[mask], current_meep_scores[mask])[0] if np.sum(mask)>2 else np.nan
                            res['Pearson_Engaging'] = pearsonr(target_eng[mask], current_meep_scores[mask])[0] if np.sum(mask)>2 else np.nan

                        if target_rel is not None:
                            mask = ~np.isnan(current_meep_scores) & ~np.isinf(current_meep_scores) & ~np.isnan(target_rel)
                            res['Spearman_Relevant'] = spearmanr(target_rel[mask], current_meep_scores[mask])[0] if np.sum(mask)>2 else np.nan
                            res['Pearson_Relevant'] = pearsonr(target_rel[mask], current_meep_scores[mask])[0] if np.sum(mask)>2 else np.nan

                        key = (ds_name, model_name, "Direct_MEEP", r_id)
                        found = False
                        for item in metrics_buffer:
                            if item.get("key") == key:
                                item.update(res)
                                found = True
                                break
                        if not found:
                            base = {"key": key, "AUC": np.nan}
                            base.update(res)
                            metrics_buffer.append(base)

                    else:
                        print(f"Warning: Direct_MEEP indices mismatch for {paths['human']}.")

                # Other methods
                method_cols = [c for c in df.columns if c.startswith("score_")]
                for method in method_cols:
                    m_name = method.replace("score_", "")
                    scores = pd.to_numeric(df[method].values, errors='coerce')

                    res = {}
                    if target_eng is not None:
                        mask = ~np.isnan(scores) & ~np.isinf(scores) & ~np.isnan(target_eng)
                        res['Spearman_Engaging'] = spearmanr(target_eng[mask], scores[mask])[0] if np.sum(mask)>2 else np.nan
                        res['Pearson_Engaging'] = pearsonr(target_eng[mask], scores[mask])[0] if np.sum(mask)>2 else np.nan

                    if target_rel is not None:
                        mask = ~np.isnan(scores) & ~np.isinf(scores) & ~np.isnan(target_rel)
                        res['Spearman_Relevant'] = spearmanr(target_rel[mask], scores[mask])[0] if np.sum(mask)>2 else np.nan
                        res['Pearson_Relevant'] = pearsonr(target_rel[mask], scores[mask])[0] if np.sum(mask)>2 else np.nan

                    key = (ds_name, model_name, m_name, r_id)
                    found = False
                    for item in metrics_buffer:
                        if item.get("key") == key:
                            item.update(res)
                            found = True
                            break
                    if not found:
                        base = {"key": key, "AUC": np.nan}
                        base.update(res)
                        metrics_buffer.append(base)

            except Exception as e: print(f"Err Empirical Human {ds_name} R{r_id}: {e}")

    # 清理并格式化
    final_list = []
    for item in metrics_buffer:
        if "key" in item:
            (d, m, met, r) = item["key"]
            del item["key"]
            item["Dataset_Type"] = "Empirical"
            item["Dataset"] = d
            item["Model"] = m
            item["Method"] = met
            item["Round"] = r
            final_list.append(item)
    return final_list

# ==========================================
# MAIN EXECUTION
# ==========================================

def main():
    all_metrics = []

    print(f"{'='*20} Starting Analysis {'='*20}")

    # 1. Iterate Synthetic
    syn_dir = Config.RESULT_DIR / "synthetic"
    if syn_dir.exists():
        for ds_path in syn_dir.iterdir():
            if ds_path.name in Config.SYN_DATASETS:
                for model_path in ds_path.iterdir():
                    print(f"Processing Synthetic: {ds_path.name} - {model_path.name}")
                    all_metrics.extend(process_synthetic(ds_path.name, model_path.name, model_path))

    # 2. Iterate Empirical
    emp_dir = Config.RESULT_DIR / "empirical"
    if emp_dir.exists():
        for ds_path in emp_dir.iterdir():
            if ds_path.name in Config.EMP_DATASETS:
                for model_path in ds_path.iterdir():
                    print(f"Processing Empirical: {ds_path.name} - {model_path.name}")
                    all_metrics.extend(process_empirical(ds_path.name, model_path.name, model_path))

    if not all_metrics:
        print("No results found.")
        return

    # 3. Aggregation
    df_raw = pd.DataFrame(all_metrics)

    # Save raw round-level data
    df_raw.to_csv(Config.OUTPUT_DIR / "all_results_raw.csv", index=False)

    # Group by (Dataset, Model, Method) and calc Mean/Std
    group_cols = ["Dataset_Type", "Dataset", "Model", "Method"]
    numeric_cols = [c for c in df_raw.columns if c not in group_cols + ["Round"]]

    df_agg = df_raw.groupby(group_cols)[numeric_cols].agg(['mean', 'std']).reset_index()

    # Flatten columns
    new_cols = []
    for c in df_agg.columns:
        if c[1] == '': new_cols.append(c[0])
        else: new_cols.append(f"{c[0]}_{c[1]}")
    df_agg.columns = new_cols

    # Save Aggregated Data
    df_agg.to_csv(Config.OUTPUT_DIR / "aggregated_results.csv", index=False)

    # 4. Generate Display Tables

    # Table 1: Synthetic Performance
    print("\n\n>>> Synthetic Data Performance (AUC & PMI Metrics) <<<")
    syn_df = df_agg[df_agg['Dataset_Type'] == "Synthetic"].copy()
    if not syn_df.empty:
        for m in ["AUC", "Spearman_PMI", "Pearson_PMI", "MSE_PMI"]:
            if f'{m}_mean' in syn_df.columns and f'{m}_std' in syn_df.columns:
                syn_df[m] = syn_df.apply(lambda x: f"{x[f'{m}_mean']:.3f} ({x[f'{m}_std']:.3f})" if pd.notnull(x[f'{m}_mean']) else "-", axis=1)
            else:
                syn_df[m] = "-"

        display_cols = ["Dataset", "Model", "Method", "AUC", "Spearman_PMI", "Pearson_PMI", "MSE_PMI"]
        existing_display_cols = [col for col in display_cols if col in syn_df.columns]
        print(syn_df[existing_display_cols].to_markdown(index=False))
        syn_df[existing_display_cols].to_csv(Config.OUTPUT_DIR / "table_synthetic.csv", index=False)

    # Table 2: Empirical Performance
    print("\n\n>>> Empirical Data Performance (AUC & Human Correlation) <<<")
    emp_df = df_agg[df_agg['Dataset_Type'] == "Empirical"].copy()
    if not emp_df.empty:
        for m in ["AUC", "Spearman_Engaging", "Pearson_Engaging", "Spearman_Relevant", "Pearson_Relevant"]:
            if f'{m}_mean' in emp_df.columns and f'{m}_std' in emp_df.columns:
                emp_df[m] = emp_df.apply(lambda x: f"{x[f'{m}_mean']:.3f} ({x[f'{m}_std']:.3f})" if pd.notnull(x[f'{m}_mean']) else "-", axis=1)
            else:
                emp_df[m] = "-"

        display_cols = ["Dataset", "Model", "Method", "AUC", "Spearman_Engaging", "Pearson_Engaging", "Spearman_Relevant", "Pearson_Relevant"]
        existing_display_cols = [col for col in display_cols if col in emp_df.columns]

        print(emp_df[existing_display_cols].to_markdown(index=False))
        emp_df[existing_display_cols].to_csv(Config.OUTPUT_DIR / "table_empirical.csv", index=False)

    print(f"\nResults saved to: {Config.OUTPUT_DIR}")

if __name__ == "__main__":
    try:
        import tabulate
    except ImportError:
        os.system("pip install tabulate")

    main()

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import warnings

# 忽略警告
warnings.filterwarnings("ignore")

# ==========================================
# 1. 全局配置 (CONFIGURATION)
# ==========================================

try:
    from google.colab import drive
    drive.mount('/content/drive')
    BASE_DRIVE_PATH = "/content/drive/MyDrive/PMIScore"
except ImportError:
    BASE_DRIVE_PATH = "./PMIScore"

class Config:
    BASE_DIR = Path(BASE_DRIVE_PATH)

    # 输入文件
    RAW_FILE = BASE_DIR / "analysis_report" / "all_results_raw.csv"
    RESULTS_DIR = BASE_DIR / "results"
    MEEP_DIR = BASE_DIR / "meep_scores"

    # 输出目录
    OUTPUT_DIR = BASE_DIR / "analysis_report" / "plots_final_v2"

    # 颜色与映射
    COLORS = {
        "PMIScore": "#1f77b4", "MINE": "#ff7f0e", "InfoNCE": "#2ca02c",
        "KDE": "#d62728", "MEEP": "#9467bd", "Direct_MEEP": "#9467bd"
    }

    METHOD_MAP = {
        "PMI_Prompt": "PMIScore", "MINE_Prompt": "MINE",
        "InfoNCE_Prompt": "InfoNCE", "KDE_Prompt": "KDE",
        "Direct_MEEP": "MEEP"
    }

    TARGET_METHODS = ["PMIScore", "MINE", "InfoNCE", "KDE", "MEEP"]
    # 定义表格中要显示的方法顺序（通常不需要 MEEP，或者你可以按需添加）
    TABLE_METHODS = ["PMIScore", "MINE", "InfoNCE", "KDE", "MEEP"]

Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']

# ==========================================
# 2. 数据处理：Pooled Statistics (保持原有逻辑不变)
# ==========================================

def load_and_aggregate_pooled():
    """
    修改后的聚合逻辑：
    1. Intra-Model Aggregation: 对每个模型内部的 5 个 Round 取 Mean。
    2. Inter-Model Statistics: 对不同模型之间计算 Mean 和 SEM。
    """
    if not Config.RAW_FILE.exists():
        print(f"Error: {Config.RAW_FILE} not found.")
        return None

    df = pd.read_csv(Config.RAW_FILE)
    df['Method_Display'] = df['Method'].map(Config.METHOD_MAP).fillna(df['Method'])
    df = df[df['Method_Display'].isin(Config.TARGET_METHODS)].copy()

    metric_cols = [c for c in df.columns if c not in ['Dataset_Type', 'Dataset', 'Model', 'Method', 'Method_Display', 'Round']]
    metric_cols = [c for c in metric_cols if pd.api.types.is_numeric_dtype(df[c])]

    # --- Step 1: Intra-Model Aggregation (消除 Round/Training 噪声) ---
    # 对每个模型取平均，得到该模型的一个稳定得分
    df_model_avg = df.groupby(['Dataset_Type', 'Dataset', 'Method_Display', 'Model'])[metric_cols].mean().reset_index()

    # --- Step 2: Inter-Model Statistics (计算泛化性能和置信度) ---
    # 计算 Mean 和 SEM (Standard Error of the Mean)
    # N = 模型数量
    final_agg = df_model_avg.groupby(['Dataset_Type', 'Dataset', 'Method_Display'])[metric_cols].agg(['mean', 'sem']).reset_index()

    # --- Step 3: Flatten and Rename Columns ---
    # 将 MultiIndex 列名展平并重命名，以适配绘图代码
    # 映射关系: mean -> _mu, sem -> _sigma

    new_cols = ['Dataset_Type', 'Dataset', 'Method_Display']
    # 临时 DataFrame 用于构建最终结果
    result_df = final_agg[new_cols].copy()

    for col in metric_cols:
        result_df[f"{col}_mu"] = final_agg[col]['mean']
        result_df[f"{col}_sigma"] = final_agg[col]['sem'] # 注意：这里存的是 SEM

    return result_df

# ==========================================
# 3. 数据处理：Raw Regression Data (保持原有逻辑不变)
# ==========================================

def get_raw_regression_data(dataset_name, n_samples=2000):
    syn_dir = Config.RESULTS_DIR / "synthetic" / dataset_name
    if not syn_dir.exists(): return None

    model_dirs = [d for d in syn_dir.iterdir() if d.is_dir()]
    if not model_dirs: return None
    target_model = model_dirs[0]
    for d in model_dirs:
        if "Qwen3-4B" in d.name: target_model = d; break

    print(f"  -> Loading raw data from {target_model.name}...")

    res_file = target_model / "round_1_test_inference_results.csv"
    if not res_file.exists(): return None
    df = pd.read_csv(res_file)

    if 'is_positive' in df.columns:
        df = df[df['is_positive'] == 1].copy()
    if df.empty: return None

    eps = 1e-12
    p_xy = df['p_xy'].clip(lower=eps)
    p_x = df['p_x'].clip(lower=eps)
    p_y = df['p_y'].clip(lower=eps)
    df['true_pmi'] = np.log(p_xy) - np.log(p_x) - np.log(p_y)

    meep_file = Config.MEEP_DIR / "synthetic" / dataset_name / target_model.name / "test_scores.csv"
    if meep_file.exists():
        try:
            m_df = pd.read_csv(meep_file)
            min_len = min(len(pd.read_csv(res_file)), len(m_df))
            m_raw = m_df.iloc[:min_len].loc[df.index]

            def parse_mean(x):
                try:
                    vals = json.loads(x)
                    valid = [v for v in vals if v is not None]
                    return np.mean(valid) if valid else np.nan
                except: return np.nan
            df['MEEP'] = m_raw['scores_raw'].apply(parse_mean)
        except: pass

    final_data = pd.DataFrame({'true_pmi': df['true_pmi']})
    rev_map = {v: k for k, v in Config.METHOD_MAP.items()}

    for disp_name in Config.TARGET_METHODS:
        raw_name = rev_map.get(disp_name, disp_name)
        for cand in [f"score_{raw_name}", raw_name, disp_name, f"score_{disp_name}"]:
            if cand in df.columns:
                final_data[disp_name] = df[cand]
                break
            if disp_name == "MEEP" and "MEEP" in df.columns:
                final_data["MEEP"] = df["MEEP"]
                break

    if len(final_data) > n_samples:
        final_data = final_data.sample(n=n_samples, random_state=42)
    return final_data.dropna()

def z_normalize(x):
    if len(x) < 2: return x
    return (x - np.nanmean(x)) / (np.nanstd(x) + 1e-9)

# ==========================================
# 4. 新增：LaTeX 表格生成函数 (完全独立逻辑)
# ==========================================

def format_latex_cell(mean, std, is_best):
    """
    格式化单元格：
    使用 \shortstack 将 std 放在均值正下方，完全不占用横向空间。
    格式：
       0.790
      (0.002)  <-- 极小字体
    """
    # 格式化数值
    val_str = f"{mean:.3f}"
    # 标准差加括号，并设为 tiny 字体，去掉前导0以进一步节省微小空间(可选)
    # std_str = f"({std:.3f})".replace("0.", ".") # 如果想极度压缩可以用这个
    std_str = f"({std:.3f})"

    # 构建堆叠结构
    # \\\\ 在 Python 字符串中转义为 LaTeX 的 \\ (换行)
    # \tiny 缩小标准差字体
    if is_best:
        # 如果是最优，只加粗上面的均值，或者上下都加粗
        # 这里只加粗均值显得更清晰，且不影响宽度
        content = f"\\makecell{{\\textbf{{{val_str}}} \\\\[-3pt] \\scriptsize {std_str}}}"
    else:
        content = f"\\makecell{{{val_str} \\\\[-3pt] \\scriptsize {std_str}}}"

    return content

def print_latex_tables():
    """
    读取原始数据，生成两个LaTeX表格：
    1. Synthetic: Dataset -> Model -> [PMIScore, MINE...] (Rho, MSE)
    2. Empirical: Dataset -> Model -> [PMIScore, MINE...] (AUC, Rho)
    """
    if not Config.RAW_FILE.exists():
        print("Raw file not found, skipping tables.")
        return

    print("\n" + "="*50)
    print(">>> GENERATING LATEX TABLES")
    print("="*50)

    # 1. 读取并过滤数据
    df = pd.read_csv(Config.RAW_FILE)
    df['Method_Display'] = df['Method'].map(Config.METHOD_MAP).fillna(df['Method'])
    df = df[df['Method_Display'].isin(Config.TABLE_METHODS)].copy()

    # ==========================
    # Table 1: Synthetic
    # ==========================
    print("\n% --- Table 1: Synthetic (Spearman & MSE) ---")
    syn_df = df[df['Dataset_Type'] == 'Synthetic'].copy()

    # 自动检测列名 (有的代码版本是 Spearman_PMI, 有的是 Pearson_PMI，优先用 Spearman)
    rho_col = 'Pearson_PMI'
    mse_col = 'MSE_PMI'

    if not syn_df.empty:
        # 聚合：按 Dataset, Model, Method 计算 Mean/Std (跨 Rounds)
        syn_agg = syn_df.groupby(['Dataset', 'Model', 'Method_Display'])[[rho_col, mse_col]].agg(['mean', 'std']).reset_index()

        datasets = sorted(syn_agg['Dataset'].unique())
        # 也可以强制指定顺序: datasets = ['diagonal', 'block', 'independent']

        # 打印表头
        print(r"\begin{table*}[!ht]")
        print(r"\centering")
        print(r"\small")
        print(r"\setlength{\tabcolsep}{3.5pt}")
        # print(r"\renewcommand{\arraystretch}{1.5}")
        print(r"\begin{tabular}{ll" + "cc" * len(Config.TABLE_METHODS) + "}")
        print(r"\toprule")

        # Header Row 1 & 2
        header1 = r"\multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} "
        header2 = " & "
        for m in Config.TABLE_METHODS:
            header1 += r"& \multicolumn{2}{c}{" + m + "} "
            header2 += r"& $\rho$ & MSE "
        print(header1 + r"\\")
        print(header2 + r"\\")
        print(r"\midrule")

        # Data Rows
        for i, ds in enumerate(datasets):
            ds_df = syn_agg[syn_agg['Dataset'] == ds]
            models = sorted(ds_df['Model'].unique())

            # Dataset Multirow
            print(r"\multirow{" + str(int(len(models) * 1.5)) + r"}{*}{" + ds.capitalize() + r"}")

            for model in models:
                row_str = f" & {model}"
                if "Llama" in model:
                    row_str = f" & Llama-3.2-3B-Instruct"
                m_df = ds_df[ds_df['Model'] == model].set_index('Method_Display')

                # 获取该行最优值用于加粗 (Rho最大, MSE最小)
                # 注意：比较时要忽略 NaN
                valid_rhos = m_df[rho_col]['mean']
                valid_mses = m_df[mse_col]['mean']
                best_rho = valid_rhos.max() if not valid_rhos.empty else -999
                best_mse = valid_mses.min() if not valid_mses.empty else 9999

                for method in Config.TABLE_METHODS:
                    if method in m_df.index:
                        # Rho
                        r_mu = m_df.loc[method, (rho_col, 'mean')]
                        r_std = m_df.loc[method, (rho_col, 'std')]
                        is_best_r = (r_mu >= best_rho - 1e-6)
                        row_str += " & " + format_latex_cell(r_mu, r_std, is_best_r)

                        # MSE
                        m_mu = m_df.loc[method, (mse_col, 'mean')]
                        m_std = m_df.loc[method, (mse_col, 'std')]
                        is_best_m = (m_mu <= best_mse + 1e-6)
                        row_str += " & " + format_latex_cell(m_mu, m_std, is_best_m)
                    else:
                        row_str += " & - & -"
                print(row_str + r"\\")

            if i < len(datasets) - 1:
                print(r"\midrule")

        print(r"\bottomrule")
        print(r"\end{tabular}")
        print(r"\caption{Performance on Synthetic datasets. Best Pearson $\rho$ (higher) and MSE (lower) per model are \textbf{bolded}.}")
        print(r"\label{tab:synthetic}")
        print(r"\end{table*}")

    # ==========================
    # Table 2: Empirical
    # ==========================
    print("\n\n% --- Table 2: Empirical (AUC & Spearman) ---")
    emp_df = df[df['Dataset_Type'] == 'Empirical'].copy()

    # 确定 Empirical 指标
    auc_col = 'AUC'
    # 有些数据集列名为 Spearman_Relevant，有的是 Spearman，尝试查找
    sp_col = 'Spearman_Relevant' if 'Spearman_Relevant' in emp_df.columns else 'Spearman'

    if not emp_df.empty:
        emp_agg = emp_df.groupby(['Dataset', 'Model', 'Method_Display'])[[auc_col, sp_col]].agg(['mean', 'std']).reset_index()
        datasets_emp = sorted(emp_agg['Dataset'].unique())

        print(r"\begin{table*}[!ht]")
        print(r"\centering")
        print(r"\small")
        print(r"\setlength{\tabcolsep}{3.5pt}")
        # print(r"\renewcommand{\arraystretch}{1.5}")
        print(r"\begin{tabular}{ll" + "cc" * len(Config.TABLE_METHODS) + "}")
        print(r"\toprule")

        header1 = r"\multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} "
        header2 = " & "
        for m in Config.TABLE_METHODS:
            header1 += r"& \multicolumn{2}{c}{" + m + "} "
            header2 += r"& AUC & $\rho$ "
        print(header1 + r"\\")
        print(header2 + r"\\")
        print(r"\midrule")

        for i, ds in enumerate(datasets_emp):
            ds_df = emp_agg[emp_agg['Dataset'] == ds]
            models = sorted(ds_df['Model'].unique())

            print(r"\multirow{" + str(int(len(models) * 1.5)) + r"}{*}{" + ds.capitalize() + r"}")

            for model in models:
                row_str = f" & {model}"
                if "Llama" in model:
                    row_str = f" & Llama-3.2-3B-Instruct"
                m_df = ds_df[ds_df['Model'] == model].set_index('Method_Display')

                valid_auc = m_df[auc_col]['mean']
                valid_sp = m_df[sp_col]['mean']
                best_auc = valid_auc.max() if not valid_auc.empty else -1
                best_sp = valid_sp.max() if not valid_sp.empty else -1

                for method in Config.TABLE_METHODS:
                    if method in m_df.index:
                        # AUC
                        a_mu = m_df.loc[method, (auc_col, 'mean')]
                        a_std = m_df.loc[method, (auc_col, 'std')]
                        is_best_a = (a_mu >= best_auc - 1e-6)
                        row_str += " & " + format_latex_cell(a_mu, a_std, is_best_a)

                        # Spearman
                        s_mu = m_df.loc[method, (sp_col, 'mean')]
                        s_std = m_df.loc[method, (sp_col, 'std')]
                        is_best_s = (s_mu >= best_sp - 1e-6)
                        row_str += " & " + format_latex_cell(s_mu, s_std, is_best_s)
                    else:
                        row_str += " & - & -"
                print(row_str + r"\\")

            if i < len(datasets_emp) - 1:
                print(r"\midrule")

        print(r"\bottomrule")
        print(r"\end{tabular}")
        print(r"\caption{Performance on Empirical datasets. Best AUC (higher) and Spearman $\rho$ (higher) per model are bolded.}")
        print(r"\label{tab:empirical}")
        print(r"\end{table*}")
    else:
        print("% No Empirical data found in raw CSV.")

# ==========================================
# 5. 绘图函数 (保持原有逻辑不变)
# ==========================================

def save_and_show(fig, filename):
    png_path = Config.OUTPUT_DIR / f"{filename}.png"
    pdf_path = Config.OUTPUT_DIR / f"{filename}.pdf"
    fig.savefig(png_path, dpi=300, bbox_inches='tight')
    fig.savefig(pdf_path, format='pdf', bbox_inches='tight')
    print(f"[Saved] {png_path}")
    plt.show()
    plt.close(fig)

def plot_figure_1_synthetic(df):
    subset = df[df['Dataset_Type'] == 'Synthetic'].copy()
    datasets = ["diagonal", "block", "independent"]
    methods = Config.TARGET_METHODS

    fig, axes = plt.subplots(1, 2, figsize=(12, 3.5))

    # Left: Pearson
    ax = axes[0]
    metric = 'Pearson_PMI'
    for i, method in enumerate(methods):
        m_data = subset[subset['Method_Display'] == method].set_index('Dataset').reindex(datasets)
        ax.bar(np.arange(3) - 0.4 + 0.8/5/2 + i*(0.8/5),
               m_data[f"{metric}_mu"], 0.8/5,
               yerr=m_data[f"{metric}_sigma"].fillna(0),
               error_kw={'elinewidth': 1, 'markeredgewidth': 1, 'ecolor': 'black'}, # Style fix
               label=method, capsize=3, color=Config.COLORS.get(method), edgecolor='black', alpha=0.9)

    ax.set_xticks(np.arange(3))
    ax.set_xticklabels([d.title() for d in datasets], fontsize=11)
    ax.set_ylabel('Pearson Correlation (↑)', fontweight='bold', fontsize=12)
    ax.set_ylim(0, 1.05)
    # ax.grid(axis='y', linestyle='--', alpha=0.3)

    # Right: MSE
    ax = axes[1]
    metric = 'MSE_PMI'
    for i, method in enumerate(methods):
        m_data = subset[subset['Method_Display'] == method].set_index('Dataset').reindex(datasets)
        ax.bar(np.arange(3) - 0.4 + 0.8/5/2 + i*(0.8/5),
               m_data[f"{metric}_mu"], 0.8/5,
               yerr=m_data[f"{metric}_sigma"].fillna(0),
               error_kw={'elinewidth': 1, 'markeredgewidth': 1, 'ecolor': 'black'}, # Style fix
               capsize=3, color=Config.COLORS.get(method), edgecolor='black', alpha=0.9)

    ax.set_xticks(np.arange(3))
    ax.set_xticklabels([d.title() for d in datasets], fontsize=11)
    ax.set_ylabel('Mean Squared Error (↓)', fontweight='bold', fontsize=12)
    # ax.grid(axis='y', linestyle='--', alpha=0.3)
    ax.set_ylim(0, 40)

    handles, labels = axes[0].get_legend_handles_labels()
    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.08), ncol=len(methods), frameon=False, fontsize=11)
    plt.tight_layout()
    save_and_show(fig, 'Figure1_Synthetic_Pearson_MSE')

def plot_figure_2_empirical(df, metric_left='AUC', metric_right='Spearman_Relevant'):
    subset = df[df['Dataset_Type'] == 'Empirical'].copy()
    datasets = sorted(subset['Dataset'].unique())
    methods = Config.TARGET_METHODS
    ds_labels = ["English", "Chinese"] if len(datasets) == 2 else datasets

    fig, axes = plt.subplots(1, 2, figsize=(12, 3.5))

    # Left: AUC
    ax = axes[0]
    metric = metric_left
    for i, method in enumerate(methods):
        m_data = subset[subset['Method_Display'] == method].set_index('Dataset').reindex(datasets)
        ax.bar(np.arange(len(datasets)) - 0.4 + 0.8/5/2 + i*(0.8/5),
               m_data[f"{metric}_mu"], 0.8/5,
               yerr=m_data[f"{metric}_sigma"].fillna(0),
               error_kw={'elinewidth': 1, 'markeredgewidth': 1, 'ecolor': 'black'}, # Style fix
               capsize=3, color=Config.COLORS.get(method), edgecolor='black', alpha=0.9)

    ax.set_xticks(np.arange(len(datasets)))
    ax.set_xticklabels(ds_labels, fontsize=11)
    ax.set_ylabel('AUC Score (↑)', fontweight='bold', fontsize=12)
    ax.set_ylim(0.4, 1.0)
    # ax.grid(axis='y', linestyle='--', alpha=0.3)

    # Right: Spearman
    ax = axes[1]
    metric = metric_right
    for i, method in enumerate(methods):
        m_data = subset[subset['Method_Display'] == method].set_index('Dataset').reindex(datasets)
        ax.bar(np.arange(len(datasets)) - 0.4 + 0.8/5/2 + i*(0.8/5),
               m_data[f"{metric}_mu"], 0.8/5,
               yerr=m_data[f"{metric}_sigma"].fillna(0),
               error_kw={'elinewidth': 1, 'markeredgewidth': 1, 'ecolor': 'black'}, # Style fix
               label=method, capsize=3, color=Config.COLORS.get(method), edgecolor='black', alpha=0.9)

    ax.set_xticks(np.arange(len(datasets)))
    ax.set_xticklabels(ds_labels, fontsize=11)
    ax.set_ylabel('Spearman Correlation (↑)', fontweight='bold', fontsize=12)
    ax.set_ylim(0, 0.6)
    # ax.grid(axis='y', linestyle='--', alpha=0.3)

    handles, labels = axes[1].get_legend_handles_labels()
    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.08), ncol=len(methods), frameon=False, fontsize=11)
    plt.tight_layout()
    save_and_show(fig, 'Figure2_Empirical_AUC_Spearman')

def plot_figure_3_regression_scatter(dataset_name):
    """单独为每个 Dataset 生成一行 1x5 的散点图"""
    print(f"Generating scatter plots for {dataset_name}...")
    df = get_raw_regression_data(dataset_name)

    if df is None or df.empty:
        print(f"Skipping {dataset_name} (No data).")
        return

    methods = Config.TARGET_METHODS
    fig, axes = plt.subplots(1, 5, figsize=(12, 3), constrained_layout=True)

    true_vals = df['true_pmi'].values
    lims = [ -4.5, 4.5]

    for i, method in enumerate(methods):
        ax = axes[i]

        if method not in df.columns:
            ax.text(0.5, 0.5, "N/A", ha='center'); ax.axis('off')
            continue

        yp = df[method].values
        mask = np.isfinite(true_vals) & np.isfinite(yp)
        yt, yp = true_vals[mask], yp[mask]

        if len(yt) > 1:
            if method == "MEEP":
                yp_plot = z_normalize(yp) * np.std(yt) + np.mean(yt)
                subtitle = "(Scaled)"
            else:
                yp_plot = yp
                subtitle = "(Raw)"
            mse_val = mean_squared_error(yt, yp_plot)
        else:
            yp_plot = yp; mse_val = np.nan; subtitle=""

        ax.scatter(yt, yp_plot, color=Config.COLORS.get(method), alpha=0.2, s=50, edgecolor='none')
        ax.plot(lims, lims, 'k--', alpha=0.6, linewidth=1)

        ax.set_title(f"{method}\nMSE={mse_val:.3f} {subtitle}", fontweight='bold', fontsize=11)
        ax.set_xlim(lims); ax.set_ylim(lims); ax.set_aspect('equal')
        ax.set_xticks([-4, -2, 0, 2, 4])
        ax.set_yticks([-4, -2, 0, 2, 4])
        # ax.grid(True, linestyle=':', alpha=0.3)

        ax.set_xlabel('Ground-Truth PMI', fontsize=10)
        if i == 0:
            ax.set_ylabel(f'{dataset_name.capitalize()}\nEstimated PMI', fontweight='bold', fontsize=11)

    save_and_show(fig, f'Figure3_Regression_Scatter_{dataset_name.capitalize()}')

# ==========================================
# MAIN EXECUTION
# ==========================================

def main():
    # 1. 新增：打印 LaTeX 表格
    print_latex_tables()

    # 2. 原有：绘图逻辑
    print("\n>>> Step 1: Loading Data & Calculating Pooled Stats (Mean + SEM)...")
    df_pooled = load_and_aggregate_pooled()

    if df_pooled is not None:
        print("\n>>> Step 2: Generating Figure 1 (Synthetic)...")
        plot_figure_1_synthetic(df_pooled)

        print("\n>>> Step 3: Generating Figure 2 (Empirical)...")
        plot_figure_2_empirical(df_pooled)
        # plot_figure_2_empirical(df_pooled, metric_right='Spearman_Engaging')

    print("\n>>> Step 4: Generating Figure 3 (Regression Scatters - Separate Files)...")
    plot_figure_3_regression_scatter("block")

    print("\n[Done] All plots generated.")

if __name__ == "__main__":
    main()